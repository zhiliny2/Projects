{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS 6 Image Classification\n",
    "\n",
    "Richard Yang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "print(tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, \\\n",
    "                                   zoom_range=0.2, horizontal_flip=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Build the training set\n",
    "trainGen = train_datagen.flow_from_directory('C:/Users\\Richa/MLcode/week6/dataset_train/dataset_train',  # this is the target directory\n",
    "                                        target_size=(64, 64),  \n",
    "                                        batch_size=32,\n",
    "                                        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the image shape of each training observation?\n",
    "\n",
    "#shape of the image\n",
    "n_shape = trainGen.image_shape\n",
    "n_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classes of the image\n",
    "n_classes = np.unique(trainGen.classes)\n",
    "n_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image shape is (64, 64, 3) and the data type is uint8. The image is a 64x64 pixel image with 3 color channels (RGB). The data type is uint8, which means that the values range from 0 to 255.\n",
    "\n",
    "There are **4** classes in the train data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Classifier Build: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN\n",
    "def build_classifier():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters = 32,\n",
    "                                    kernel_size = (3,3),\n",
    "                                    input_shape = n_shape,\n",
    "                                    activation = 'relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters = 64,\n",
    "                                    kernel_size = (3,3),\n",
    "                                    activation = 'relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(units = 128, activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(units = len(n_classes), activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1605760   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,625,668\n",
      "Trainable params: 1,625,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the classifier\n",
    "\n",
    "model1 = build_classifier()\n",
    "model1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Runs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3/3 [==============================] - 2s 233ms/step - loss: 1.3687 - accuracy: 0.4091\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 1s 335ms/step - loss: 0.7017 - accuracy: 0.7727\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 1s 288ms/step - loss: 0.3325 - accuracy: 0.9318\n"
     ]
    }
   ],
   "source": [
    "# a) Use .fit() with the training set. For the first run, use the following parameters: steps_per_epoch = 3, epochs = 3\n",
    "\n",
    "model_my = model1.fit_generator(trainGen, steps_per_epoch = 3, epochs = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model1.save('model_my.h5')\n",
    "print(\"Saved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n",
      "C:/Users/Richa/MLcode/week6/dataset_test\\1022.png\n",
      "C:/Users/Richa/MLcode/week6/dataset_test\\1053.png\n",
      "C:/Users/Richa/MLcode/week6/dataset_test\\4011.png\n",
      "C:/Users/Richa/MLcode/week6/dataset_test\\4053.png\n",
      "C:/Users/Richa/MLcode/week6/dataset_test\\6023.png\n",
      "C:/Users/Richa/MLcode/week6/dataset_test\\6051.png\n",
      "C:/Users/Richa/MLcode/week6/dataset_test\\C014.png\n",
      "C:/Users/Richa/MLcode/week6/dataset_test\\C033.png\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([0], dtype=int64),\n",
       " array([2], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([1], dtype=int64),\n",
       " array([3], dtype=int64)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c) Predict using the model built in step 2.\n",
    "\n",
    "# load the model\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('model_my.h5')\n",
    "print(\"Saved model\")\n",
    "\n",
    "# test data path\n",
    "img_dir = \"C:/Users/Richa/MLcode/week6/dataset_test\" # Enter Directory of test set\n",
    "\n",
    "# iterate over each test image\n",
    "data_path = os.path.join(img_dir, '*g')\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "# print the files in the dataset_test folder \n",
    "for f in files:\n",
    "    print(f)\n",
    "    \n",
    "# make a prediction and add to results \n",
    "data = []\n",
    "results = []\n",
    "for f1 in files:\n",
    "    img = image.load_img(f1, target_size = (64, 64))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    data.append(img)\n",
    "    result = model.predict(img)\n",
    "    r = np.argmax(result, axis=1)\n",
    "    results.append(r)\n",
    "\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Determine accuracy.\n",
    "\n",
    "Note: To determine accuracy, you will need to check the labels given to each class in the training data and manually label your test data. This will require you to\n",
    "\n",
    "Look into the training data(images) in the dataset_train folder, and then determine how a category was coded in keras using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category 1': 0, 'category 2': 1, 'category 3': 2, 'category 4': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check category labels in training_set\n",
    "trainGen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# according the results, the predicted labels are:\n",
    "predicted_label = [0,0,0,2,1,1,1,3]\n",
    "\n",
    "#Manually checking the labels of the test data\n",
    "actual_label = [0,0,2,2,1,1,3,3]\n",
    "\n",
    "# # Compare the predicted values to the actual values for the test set and calculate accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(actual_label, predicted_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Run this process for the following combinations:\n",
    "\n",
    "* (steps_per_epoch: 1, epochs: 1)\n",
    "* (steps_per_epoch: 1, epochs: 2)\n",
    "* (steps_per_epoch: 1, epochs: 3)\n",
    "* (steps_per_epoch: 2, epochs: 4)\n",
    "* (steps_per_epoch: 2, epochs: 5)\n",
    "* (steps_per_epoch: 2, epochs: 6)\n",
    "* (steps_per_epoch: 3, epochs: 7)\n",
    "* (steps_per_epoch: 3, epochs: 8)\n",
    "* (steps_per_epoch: 5, epochs: 9)\n",
    "* (steps_per_epoch: 5, epochs: 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 1.4191 - accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.3606 - accuracy: 0.2812\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1.7013 - accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1_2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4157 - accuracy: 0.2500\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 2.1911 - accuracy: 0.2500\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 1.2355 - accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1_3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_train_function.<locals>.train_function at 0x000002B4110054C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_train_function.<locals>.train_function at 0x000002B4110054C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 275ms/step - loss: 1.7263 - accuracy: 0.3036\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 1s 306ms/step - loss: 1.3793 - accuracy: 0.2857\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 1s 278ms/step - loss: 0.9764 - accuracy: 0.6429\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 1s 231ms/step - loss: 0.7950 - accuracy: 0.8571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_2_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_2_4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/5\n",
      "2/2 [==============================] - 1s 207ms/step - loss: 1.6673 - accuracy: 0.2321\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 1s 213ms/step - loss: 2.0298 - accuracy: 0.3036\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 1.4164 - accuracy: 0.4643\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 1s 223ms/step - loss: 0.7459 - accuracy: 0.8214\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 1s 227ms/step - loss: 0.5692 - accuracy: 0.8929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_2_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_2_5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/6\n",
      "2/2 [==============================] - 1s 300ms/step - loss: 2.5554 - accuracy: 0.2857\n",
      "Epoch 2/6\n",
      "2/2 [==============================] - 1s 321ms/step - loss: 1.4357 - accuracy: 0.4286\n",
      "Epoch 3/6\n",
      "2/2 [==============================] - 1s 271ms/step - loss: 0.9503 - accuracy: 0.6406\n",
      "Epoch 4/6\n",
      "2/2 [==============================] - 1s 296ms/step - loss: 0.8020 - accuracy: 0.7031\n",
      "Epoch 5/6\n",
      "2/2 [==============================] - 1s 343ms/step - loss: 0.6166 - accuracy: 0.8929\n",
      "Epoch 6/6\n",
      "2/2 [==============================] - 1s 294ms/step - loss: 0.4027 - accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_2_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_2_6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 1/7\n",
      "3/3 [==============================] - 2s 289ms/step - loss: 1.4872 - accuracy: 0.2614\n",
      "Epoch 2/7\n",
      "3/3 [==============================] - 1s 258ms/step - loss: 1.0401 - accuracy: 0.6591\n",
      "Epoch 3/7\n",
      "3/3 [==============================] - 1s 329ms/step - loss: 0.5811 - accuracy: 0.9091\n",
      "Epoch 4/7\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 0.2845 - accuracy: 0.9432\n",
      "Epoch 5/7\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.1880 - accuracy: 0.9545\n",
      "Epoch 6/7\n",
      "3/3 [==============================] - 1s 271ms/step - loss: 0.1837 - accuracy: 0.9318\n",
      "Epoch 7/7\n",
      "3/3 [==============================] - 1s 290ms/step - loss: 0.1557 - accuracy: 0.9545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_3_7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_3_7\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Epoch 1/8\n",
      "3/3 [==============================] - 2s 293ms/step - loss: 1.7675 - accuracy: 0.2727\n",
      "Epoch 2/8\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 1.0341 - accuracy: 0.6136\n",
      "Epoch 3/8\n",
      "3/3 [==============================] - 1s 315ms/step - loss: 0.6127 - accuracy: 0.8750\n",
      "Epoch 4/8\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.3673 - accuracy: 0.8977\n",
      "Epoch 5/8\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.2391 - accuracy: 0.9205\n",
      "Epoch 6/8\n",
      "3/3 [==============================] - 1s 266ms/step - loss: 0.1661 - accuracy: 0.9545\n",
      "Epoch 7/8\n",
      "3/3 [==============================] - 1s 309ms/step - loss: 0.1749 - accuracy: 0.9432\n",
      "Epoch 8/8\n",
      "3/3 [==============================] - 1s 321ms/step - loss: 0.0991 - accuracy: 0.9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_3_8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_3_8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/9\n",
      "3/5 [=================>............] - ETA: 0s - loss: 1.8352 - accuracy: 0.2500WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 45 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 45 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 550ms/step - loss: 1.8352 - accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_5_9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_5_9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Epoch 1/10\n",
      "3/5 [=================>............] - ETA: 0s - loss: 1.3459 - accuracy: 0.4545WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 130ms/step - loss: 1.3459 - accuracy: 0.4545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_5_10\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_5_10\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = [1, 1, 1, 2, 2, 2, 3, 3, 5, 5]\n",
    "epochs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "score = []\n",
    "\n",
    "for i in range(0,len(steps_per_epoch)):\n",
    "    model1 = build_classifier()\n",
    "    # compile the model\n",
    "    model1.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    model1.fit_generator(trainGen,steps_per_epoch=steps_per_epoch[i], epochs=epochs[i])\n",
    "    model_name = \"model_\" +str(steps_per_epoch[i])+\"_\"+str(epochs[i])\n",
    "    model1.save(model_name)\n",
    "    \n",
    "    model = load_model(model_name)\n",
    "# make a prediction and add to results \n",
    "    data = []\n",
    "    results = []\n",
    "    for f1 in files:\n",
    "        img = image.load_img(f1, target_size = (64, 64))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis = 0)\n",
    "        data.append(img)\n",
    "        result = model.predict(img)\n",
    "        r = np.argmax(result, axis=1)\n",
    "        results.append(r)\n",
    "\n",
    "    results = list(np.concatenate(results))\n",
    "    \n",
    "    score.append([steps_per_epoch[i], epochs[i], results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_accuracy = []\n",
    "for idx in range(0,len(steps_per_epoch)):\n",
    "    accuracy=0\n",
    "    for j in range(0,len(actual_label)):\n",
    "        if score[idx][2][j] == actual_label[j]:\n",
    "            accuracy += 1\n",
    "    list_accuracy.append(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain what the above for loop does:  \n",
    "\n",
    "The for loop runs the model with different steps_per_epoch and epochs and prints out the accuracy of each model.\n",
    "\n",
    "The steps_per_epoch is the number of batches of training images that go through the model before the epoch is considered finished. The epoch is the number of times the model goes through the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps_per_epoch</th>\n",
       "      <th>epoch</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   steps_per_epoch  epoch  accuracy\n",
       "0                1      1     0.250\n",
       "1                1      2     0.625\n",
       "2                1      3     0.250\n",
       "3                2      4     0.875\n",
       "4                2      5     0.750\n",
       "5                2      6     0.875\n",
       "6                3      7     0.750\n",
       "7                3      8     0.875\n",
       "8                5      9     0.625\n",
       "9                5     10     0.500"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'steps_per_epoch' : steps_per_epoch, 'epoch': epochs,'accuracy' : list_accuracy}\n",
    "df_accuracy = pd.DataFrame(data)\n",
    "df_accuracy['accuracy'] = (df_accuracy['accuracy']/8)\n",
    "df_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the accuracy increases as the steps_per_epoch and epochs increase. This is because the model is trained more times and the model is able to learn more from the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Questions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Discuss the effect of the following on accuracy and loss (train & test): \n",
    "\n",
    "- Increasing the steps_per_epoch\n",
    "- Increasing the number of epochs\n",
    "\n",
    "Increasing the steps_per_epoch increases the accuracy and decreases the loss. Increasing the number of epochs increases the accuracy and decreases the loss.\n",
    "\n",
    "Increasing the number of epochs increases the accuracy and decreases the loss. Increasing the steps_per_epoch increases the accuracy and decreases the loss."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Name two uses of zero padding in CNN.\n",
    "\n",
    "- Zero padding can be used to preserve the size of the input image. This is important because the image needs to be the same size as the output of the convolutional layers.\n",
    "\n",
    "- It is also used to prevent the shrinking of the image as it goes through the convolutional layers. This is important because the image needs to be the same size as the output of the convolutional layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is the use of a 1 x 1 kernel in CNN? \n",
    "\n",
    "- Dimensionality reduction. It is used to reduce the number of channels in the image. By using a 1x1 kernel, the convolutional layer can transform the input feature map from one depth (number of channels) to another depth, effectively reducing or increasing the number of channels. This can be helpful in reducing the computational cost of subsequent layers or adjusting the complexity of the network.\n",
    "\n",
    "- Non-linearity. It is used to introduce non-linearity into the network. By using a 1x1 kernel, the convolutional layer can introduce non-linearity into the network. This can be helpful in increasing the complexity of the network.\n",
    "\n",
    "- Improve Network Architecture. The presence of 1x1 convolutional layers enables the network to have more flexibility and expressiveness."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What are the advantages of a CNN over a fully connected DNN for this image classification problem?\n",
    "\n",
    "- Local receptive fields: CNNs capture spatial relationships by connecting each neuron to a small region of the input image, enabling effective extraction of local patterns.\n",
    "\n",
    "- Parameter sharing: Sharing weights across regions reduces parameters, making CNNs more efficient and enabling the learning of translation-invariant features.\n",
    "\n",
    "- Pooling layers: Pooling summarizes important features, reduces spatial dimensions, and enhances robustness to small image transformations.\n",
    "\n",
    "- Hierarchical representation learning: CNNs learn hierarchical features, progressing from low-level to high-level representations, enabling the modeling of complex concepts.\n",
    "\n",
    "- Parameter efficiency: CNNs achieve high accuracy with fewer parameters by exploiting local structure, reducing computational complexity, and memory requirements.\n",
    "\n",
    "In a fully connected layer, every neuron is connected to all neurons in the previous layer, with each connection having its own weight. This type of connection pattern is not specific to any particular features in the data and lacks assumptions about feature relationships. However, it is computationally and memory-intensive due to the large number of weights required to connect every neuron. \n",
    "\n",
    "In this problem, the CNN is more efficient than the fully connected DNN because it is a image classification problem. There are a lot of pixels in an image. There are also spatial relationships between pixels in an image. In addition, CNN will need less computational power and memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
