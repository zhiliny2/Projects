{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a481c810",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "\n",
    "The self-attention mechanism, also known as intra-attention or scaled dot-product attention, is a key component in transformer-based models. It allows the model to capture relationships between different words within the same sentence or sequence by assigning attention weights.\n",
    "\n",
    "To explain the self-attention mechanism, let's consider the sample sentence \"The cat sat on the mat.\" We'll use a simplified version of the self-attention mechanism using dot products.\n",
    "\n",
    "   **Embedding**: First, each word in the sentence is transformed into a word embedding, which represents the word's meaning in a vector space.\n",
    "\n",
    "   **Query, Key, and Value**: The word embeddings are used to generate three types of vectors: query, key, and value. These vectors are obtained by multiplying the word embeddings by learnable weights (often referred to as matrices) specific to each type. Each word embedding is transformed into query, key, and value vectors. These vectors allow the model to compare and relate different words in the sentence. \n",
    "   \n",
    "   For example, the word \"cat\" will have query, key, and value vectors associated with it. \n",
    "    \n",
    "   The query is a vector representation that encodes the information or context that we want to retrieve from the input sequence. (The word or token in the English sentence that we want to translate at a particular time step, such as \"cat\".)\n",
    "    \n",
    "   The key vector represents the elements or features of the input sequence that we want to attend to or retrieve information from.(The representations or embeddings of the words in the input English sentence, for example, \"I\", \"have\", \"a\", \"cat\".)\n",
    "\n",
    "   The value vector represents the information or content associated with each element in the input sequence. (The corresponding representations or embeddings of the words in the input English sentence, such as the word embeddings of \"I\", \"have\", \"a\", \"cat\".)\n",
    "    \n",
    "   The key vectors capture the relationships and importance of different elements, while the value vectors hold the actual information or content associated with those elements. The attention mechanism utilizes the key and value vectors together to compute attention weights and generate the context vector.    \n",
    "\n",
    "   **Similarity Calculation**: To compute the attention weights, we calculate the similarity between each query vector and all key vectors. This can be done by taking the dot product between the query and key vectors and scaling the result by the square root of the dimension of the key vectors.\n",
    "\n",
    "   **Attention Weights** : The similarity scores are further processed using the softmax function to obtain attention weights. The softmax ensures that the attention weights sum up to 1 and represent the importance or relevance of each word in the sentence.\n",
    "\n",
    "   **Context Vector**: The attention weights are used to weigh the value vectors. The value vectors hold the information that the model will focus on for each word. The weighted sum of the value vectors, using the attention weights as coefficients, produces the context vector, which captures the relevant information from the entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28dc60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf18a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\n",
      "[[ 0.2  0.3 -0.1]\n",
      " [ 0.5 -0.2  0.4]\n",
      " [-0.3  0.1  0.6]\n",
      " [ 0.4  0.5 -0.3]]\n",
      "\n",
      "Context Vector:\n",
      "[[ 0.06301839 -0.13436515  0.01266342]\n",
      " [-0.29911514 -0.23966649  0.09011734]\n",
      " [-0.10170689 -0.14678699  0.18305492]\n",
      " [ 0.21985998 -0.09596336 -0.04224468]]\n",
      "\n",
      "Attention Weights:\n",
      "[[0.22933771 0.17550718 0.4118571  0.18329801]\n",
      " [0.25683411 0.27349775 0.09179122 0.37787692]\n",
      " [0.20338818 0.41051068 0.13779926 0.24830188]\n",
      " [0.1920107  0.10477828 0.57860311 0.1246079 ]]\n"
     ]
    }
   ],
   "source": [
    "def self_attention(sentence, W_query, W_key, W_value):\n",
    "    # Compute query, key, and value vectors\n",
    "    queries = np.dot(sentence, W_query)\n",
    "    keys = np.dot(sentence, W_key)\n",
    "    values = np.dot(sentence, W_value)\n",
    "    \n",
    "    # Compute similarity scores using dot product and scaling\n",
    "    scores = np.dot(queries, keys.T) / np.sqrt(keys.shape[-1])\n",
    "    \n",
    "    # Compute attention weights using softmax\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute context vector using attention weights and values\n",
    "    context_vector = np.dot(attention_weights, values)\n",
    "    \n",
    "    return context_vector, attention_weights\n",
    "\n",
    "\n",
    "# Sample sentence\n",
    "sentence = np.array([[0.2, 0.3, -0.1],\n",
    "                     [0.5, -0.2, 0.4],\n",
    "                     [-0.3, 0.1, 0.6],\n",
    "                     [0.4, 0.5, -0.3]])\n",
    "\n",
    "# Learnable weight matrices\n",
    "W_query = np.random.randn(sentence.shape[1], sentence.shape[1])\n",
    "W_key = np.random.randn(sentence.shape[1], sentence.shape[1])\n",
    "W_value = np.random.randn(sentence.shape[1], sentence.shape[1])\n",
    "\n",
    "# Apply self-attention mechanism\n",
    "context_vector, attention_weights = self_attention(sentence, W_query, W_key, W_value)\n",
    "\n",
    "# Print results\n",
    "print(\"Sentence:\")\n",
    "print(sentence)\n",
    "print(\"\\nContext Vector:\")\n",
    "print(context_vector)\n",
    "print(\"\\nAttention Weights:\")\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227bd6b",
   "metadata": {},
   "source": [
    "## Self Attention Keras \n",
    "\n",
    "We will use Keras Embedding layer to generate word embeddings. \n",
    "\n",
    "We first tokenize the sentence and convert it to sequences of word indices using the tokenizer. Then, we create an embedding layer with the specified input dimension (vocab_size) and output dimension (embedding_dim).\n",
    "\n",
    "We apply the embedding layer to the padded sequences, resulting in embedded sequences. These embedded sequences represent the word embeddings for the sentence.\n",
    "\n",
    "Finally, we pass the embedded sequences to the self_attention function along with the learnable weight matrices to obtain the context vector and attention weights. The results are printed out for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8ea00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(sentence, W_query, W_key, W_value):\n",
    "    # Compute query, key, and value vectors\n",
    "    queries = tf.matmul(sentence, W_query)\n",
    "    keys = tf.matmul(sentence, W_key)\n",
    "    values = tf.matmul(sentence, W_value)\n",
    "    \n",
    "    # Compute similarity scores using dot product and scaling\n",
    "    scores = tf.matmul(queries, keys, transpose_b=True) / tf.sqrt(tf.cast(keys.shape[-1], tf.float32))\n",
    "    \n",
    "    # Compute attention weights using softmax\n",
    "    attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "    \n",
    "    # Compute context vector using attention weights and values\n",
    "    context_vector = tf.matmul(attention_weights, values)\n",
    "    \n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bac36f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Max\n",
      "Sentence:\n",
      "I love natural language processing\n",
      "\n",
      "Context Vector:\n",
      "tf.Tensor(\n",
      "[[[-0.21116059  0.1338353   0.13333878  0.05117755  0.30933744\n",
      "   -0.0305737  -0.06057891  0.08202386  0.0193999  -0.13128015\n",
      "    0.19713676  0.17998238  0.00066628 -0.03445598  0.11257076\n",
      "   -0.11821473  0.156113    0.05705294 -0.05195492  0.05657417\n",
      "    0.07310655  0.06868353 -0.01852534  0.01886342  0.05810512\n",
      "   -0.16444445  0.0838085  -0.10200395 -0.07993244  0.08214889\n",
      "   -0.09336944  0.13650562  0.09028789 -0.05318436 -0.00255455\n",
      "    0.16088843 -0.06157837 -0.01500967 -0.02165653 -0.026959\n",
      "    0.03200218 -0.06000656  0.11999457 -0.12722476 -0.18554088\n",
      "   -0.0638378  -0.14260234 -0.11518177  0.22430842  0.12695412\n",
      "    0.00529285  0.00837352  0.00952021 -0.10221426 -0.07873211\n",
      "    0.05573184 -0.10702249 -0.10238901  0.02699872 -0.05801383\n",
      "   -0.1904501   0.09435353  0.31905353 -0.08835943  0.10708644\n",
      "   -0.08998151 -0.13991088 -0.15970846 -0.09349819 -0.16864042\n",
      "   -0.2549234   0.00560717  0.24486013 -0.1383079  -0.01420523\n",
      "   -0.07286133  0.0084863   0.01845413  0.02410872 -0.05814372\n",
      "    0.02436167  0.18424085  0.15940143 -0.02441842 -0.0890841\n",
      "   -0.02281413 -0.04349089 -0.00124475  0.07153523 -0.04095158\n",
      "   -0.03550455  0.23551801  0.3197869   0.14180358 -0.1194448\n",
      "   -0.07235615 -0.03976199 -0.06378539  0.13347334 -0.11902641]\n",
      "  [-0.22076997  0.13907607  0.13094623  0.05018942  0.31038263\n",
      "   -0.03596512 -0.05791783  0.08170155  0.0287765  -0.13057674\n",
      "    0.20421329  0.18166843  0.00072473 -0.03969384  0.1048605\n",
      "   -0.12315498  0.14348036  0.04636027 -0.0566967   0.05658377\n",
      "    0.07750446  0.06287724 -0.01471031  0.01713467  0.05933645\n",
      "   -0.16017084  0.08049171 -0.09818021 -0.08142092  0.08124625\n",
      "   -0.08717186  0.14297202  0.08405095 -0.0557576  -0.00310609\n",
      "    0.15454315 -0.06117126 -0.02111238 -0.02399001 -0.02002417\n",
      "    0.03507715 -0.05860229  0.10753094 -0.12654078 -0.18871978\n",
      "   -0.05448392 -0.1443947  -0.10886969  0.22691163  0.12437954\n",
      "    0.00440446  0.00600686  0.00767933 -0.09749168 -0.08283494\n",
      "    0.05518133 -0.11298074 -0.10852788  0.03378072 -0.05597931\n",
      "   -0.19604886  0.09173746  0.32134783 -0.08151044  0.10911657\n",
      "   -0.08368791 -0.14436114 -0.1626694  -0.09550314 -0.16763398\n",
      "   -0.24160314 -0.00896309  0.24556231 -0.1436756  -0.0117319\n",
      "   -0.07358853  0.01011716  0.02281273  0.02985528 -0.05537844\n",
      "    0.02892711  0.18700932  0.16356832 -0.0264451  -0.08928092\n",
      "   -0.03275395 -0.02933139 -0.00287907  0.06809332 -0.0384775\n",
      "   -0.03079807  0.2370614   0.32318562  0.13899292 -0.10822864\n",
      "   -0.07306521 -0.03573204 -0.06797902  0.13416374 -0.11763985]\n",
      "  [-0.20680682  0.15481563  0.12759408  0.05082333  0.33487278\n",
      "   -0.01812517 -0.07403778  0.07375677  0.05692209 -0.14676021\n",
      "    0.20119739  0.18216117 -0.0036953  -0.03455208  0.09161504\n",
      "   -0.11287522  0.11791468  0.0394388  -0.04918763  0.04606454\n",
      "    0.09144834  0.05439621 -0.01555983  0.00171629  0.04519078\n",
      "   -0.16213298  0.05345196 -0.08486921 -0.07849835  0.08440735\n",
      "   -0.08417119  0.15740949  0.06029696 -0.0414013   0.00179436\n",
      "    0.14447576 -0.05292215 -0.03250561 -0.03763299 -0.00557249\n",
      "    0.04784852 -0.07571258  0.09603586 -0.11740085 -0.19770464\n",
      "   -0.0599556  -0.15606718 -0.10267268  0.20402601  0.12831262\n",
      "   -0.00237357  0.02823425 -0.0174135  -0.08724053 -0.1105311\n",
      "    0.03744794 -0.1259337  -0.12438139  0.03785818 -0.03991684\n",
      "   -0.22928172  0.09772467  0.3388394  -0.09445266  0.12087108\n",
      "   -0.09615229 -0.16792132 -0.15990515 -0.11424233 -0.15405086\n",
      "   -0.2306462  -0.02734539  0.26947296 -0.1451554  -0.02358235\n",
      "   -0.06191154  0.02227829  0.00485478  0.05209145 -0.05515476\n",
      "    0.02773776  0.19636154  0.18831734 -0.00468943 -0.08270069\n",
      "   -0.05325176 -0.01238271 -0.01486546  0.07443774 -0.04844104\n",
      "   -0.03071161  0.2341685   0.32798105  0.14843203 -0.1425677\n",
      "   -0.07917131 -0.03689104 -0.08096486  0.13975468 -0.10491516]\n",
      "  [-0.24525973  0.14577162  0.14542031  0.04142759  0.30365697\n",
      "   -0.040256   -0.04303826  0.08479743  0.03589441 -0.15488908\n",
      "    0.20400341  0.15902472 -0.03956402 -0.03057676  0.08294973\n",
      "   -0.14065452  0.11216599  0.0565101  -0.09071525  0.06694956\n",
      "    0.07860418  0.07253096 -0.00065885  0.04367028  0.0620752\n",
      "   -0.17897177  0.06164874 -0.11059782 -0.09598801  0.06650902\n",
      "   -0.06646398  0.16822901  0.05643179 -0.03101413  0.01435091\n",
      "    0.15197194 -0.08551633 -0.04652957 -0.01508607 -0.03750592\n",
      "    0.03214459 -0.07383439  0.06121165 -0.13259155 -0.18353738\n",
      "   -0.05467625 -0.1552024  -0.09056578  0.22466454  0.13870166\n",
      "    0.01626457  0.0115518  -0.00347576 -0.06914835 -0.09401111\n",
      "    0.03774496 -0.14433594 -0.14041343  0.04854018 -0.05281821\n",
      "   -0.21211621  0.08765626  0.3109305  -0.05218478  0.1072477\n",
      "   -0.0626768  -0.14147906 -0.17399813 -0.11977441 -0.16693881\n",
      "   -0.21186765 -0.04518264  0.27230954 -0.1223709  -0.00158997\n",
      "   -0.09512243  0.01188402  0.0357582   0.0733037  -0.05370598\n",
      "    0.05727851  0.21743236  0.16056207 -0.00794737 -0.09748036\n",
      "   -0.03385896 -0.00613731  0.02287977  0.07430668 -0.03542435\n",
      "   -0.03037389  0.22249302  0.32338595  0.12659138 -0.09200932\n",
      "   -0.08443573 -0.05408561 -0.0832481   0.15145233 -0.13541947]\n",
      "  [-0.22316922  0.14982256  0.1344217   0.04797006  0.3206249\n",
      "   -0.02819087 -0.05873933  0.07783742  0.04446651 -0.14574358\n",
      "    0.2004092   0.17157592 -0.01790373 -0.03373269  0.09224682\n",
      "   -0.12224687  0.11462554  0.0431811  -0.06721219  0.05318582\n",
      "    0.08732815  0.06014941 -0.00456226  0.02187455  0.05436912\n",
      "   -0.1678669   0.05917232 -0.09251143 -0.08632325  0.07807335\n",
      "   -0.07414941  0.15826528  0.06230983 -0.04242278  0.00544001\n",
      "    0.14525855 -0.06658354 -0.0395996  -0.02446696 -0.01627699\n",
      "    0.04286439 -0.0737423   0.07867602 -0.12546495 -0.19104727\n",
      "   -0.0559774  -0.15465985 -0.09747222  0.2144309   0.1309555\n",
      "    0.00194231  0.02040715 -0.00670856 -0.08270687 -0.10062726\n",
      "    0.03797025 -0.13205737 -0.13087879  0.04158236 -0.04583889\n",
      "   -0.21809393  0.08923036  0.3300509  -0.07393494  0.11301257\n",
      "   -0.08011947 -0.15748225 -0.1636169  -0.11307283 -0.16102181\n",
      "   -0.22245765 -0.0347843   0.26900345 -0.13640127 -0.01510689\n",
      "   -0.07830572  0.01516899  0.01756771  0.06025232 -0.05463233\n",
      "    0.04167924  0.20423296  0.17664197 -0.01026344 -0.09044255\n",
      "   -0.04637893 -0.00718357  0.00171353  0.06987637 -0.03920335\n",
      "   -0.02562909  0.23189878  0.32621396  0.14115085 -0.11575811\n",
      "   -0.07948762 -0.04580849 -0.07861438  0.14342669 -0.11749069]]], shape=(1, 5, 100), dtype=float32)\n",
      "\n",
      "Attention Weights:\n",
      "tf.Tensor(\n",
      "[[[0.20218158 0.17652921 0.18248683 0.21848866 0.22031368]\n",
      "  [0.19855663 0.18292926 0.19267575 0.2177241  0.20811422]\n",
      "  [0.21564053 0.19472493 0.21122392 0.17392066 0.20448999]\n",
      "  [0.17849977 0.23640022 0.19289492 0.20093013 0.19127494]\n",
      "  [0.20295441 0.21146977 0.20147952 0.19116586 0.1929305 ]]], shape=(1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Sample English sentence\n",
    "sentence = \"I love natural language processing\"\n",
    "\n",
    "# Prepare tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([sentence])\n",
    "\n",
    "# Convert sentence to sequence of word indices\n",
    "sequences = tokenizer.texts_to_sequences([sentence])\n",
    "padded_sequences = pad_sequences(sequences, padding=\"post\")\n",
    "\n",
    "# Create word index and vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "# Define embedding dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "# Create embedding layer\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "\n",
    "# Apply embedding layer to the padded sequences\n",
    "embedded_sequences = embedding_layer(padded_sequences)\n",
    "\n",
    "# Learnable weight matrices\n",
    "W_query = np.random.randn(embedding_dim, embedding_dim)\n",
    "W_key = np.random.randn(embedding_dim, embedding_dim)\n",
    "W_value = np.random.randn(embedding_dim, embedding_dim)\n",
    "\n",
    "# Apply self-attention mechanism\n",
    "context_vector, attention_weights = self_attention(embedded_sequences, W_query, W_key, W_value)\n",
    "\n",
    "# Print results\n",
    "print(\"Sentence:\")\n",
    "print(sentence)\n",
    "print(\"\\nContext Vector:\")\n",
    "print(context_vector)\n",
    "print(\"\\nAttention Weights:\")\n",
    "print(attention_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
