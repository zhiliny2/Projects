{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Automatic Differentiation \n",
    "\n",
    "TensorFlow provides functions to compute the derivatives for a given TensorFlow computation graph, adding operations to the graph. The optimizer classes automatically compute derivatives on your graph, but creators of new Optimizers or expert users can call the lower-level functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Computation\n",
    "\n",
    "In order to compute gradient of function with respect to a variable you have to define both. Also you have to specify value at which you want to compute the gradient. \n",
    "\n",
    "<code>GradientTape</code> records operations for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(5.0)\n",
    "\n",
    "#compute gradient of y=x**2\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x * x\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 75.      300.      696.07074]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[5.0, 10, 15.23232]])\n",
    "\n",
    "#compute gradient of y=x**3\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x * x * x\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "#compute gradient of y=x**2+x+1 with respect to x at 3\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2 + x - 1\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "TensorFlow uses reverse mode automatic differentiation for it's gradients operation and finite difference method for tests that check validity of gradient operation. [Reverse mode automatic differentiation](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation)  uses an extension of the forward mode computational graph to enable the computation of a gradient by a reverse traversal of the graph.\n",
    "\n",
    "Optimize the following:  $min (x + 1)^2$\n",
    "\n",
    "$\\frac{d}{dx} (x+1)^2 = 2*(x+1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [3.0, 11.0]\n",
      "Epoch : 1 [2.93, 10.514901]\n",
      "Epoch : 2 [2.8614001, 10.049011]\n",
      "Epoch : 3 [2.794172, 9.601569]\n",
      "Epoch : 4 [2.7282887, 9.171848]\n",
      "Epoch : 5 [2.663723, 8.759144]\n",
      "Epoch : 6 [2.6004486, 8.362782]\n",
      "Epoch : 7 [2.5384398, 7.982116]\n",
      "Epoch : 8 [2.477671, 7.6165237]\n",
      "Epoch : 9 [2.4181175, 7.26541]\n",
      "Epoch : 10 [2.3597553, 6.9282007]\n",
      "Epoch : 11 [2.30256, 6.6043434]\n",
      "Epoch : 12 [2.2465088, 6.293311]\n",
      "Epoch : 13 [2.1915786, 5.9945955]\n",
      "Epoch : 14 [2.137747, 5.7077093]\n",
      "Epoch : 15 [2.0849922, 5.432184]\n",
      "Epoch : 16 [2.0332923, 5.1675696]\n",
      "Epoch : 17 [1.9826264, 4.913434]\n",
      "Epoch : 18 [1.9329739, 4.669362]\n",
      "Epoch : 19 [1.8843144, 4.434955]\n",
      "Epoch : 20 [1.8366281, 4.2098308]\n",
      "Epoch : 21 [1.7898955, 3.9936216]\n",
      "Epoch : 22 [1.7440976, 3.785974]\n",
      "Epoch : 23 [1.6992157, 3.5865495]\n",
      "Epoch : 24 [1.6552314, 3.3950224]\n",
      "Epoch : 25 [1.6121267, 3.2110791]\n",
      "Epoch : 26 [1.5698842, 3.0344205]\n",
      "Epoch : 27 [1.5284865, 2.8647575]\n",
      "Epoch : 28 [1.4879167, 2.7018127]\n",
      "Epoch : 29 [1.4481584, 2.545321]\n",
      "Epoch : 30 [1.4091952, 2.3950262]\n",
      "Epoch : 31 [1.3710113, 2.250683]\n",
      "Epoch : 32 [1.333591, 2.1120558]\n",
      "Epoch : 33 [1.2969191, 1.9789183]\n",
      "Epoch : 34 [1.2609807, 1.8510531]\n",
      "Epoch : 35 [1.225761, 1.7282512]\n",
      "Epoch : 36 [1.1912458, 1.6103123]\n",
      "Epoch : 37 [1.1574209, 1.497044]\n",
      "Epoch : 38 [1.1242725, 1.3882611]\n",
      "Epoch : 39 [1.091787, 1.2837858]\n",
      "Epoch : 40 [1.0599512, 1.1834477]\n",
      "Epoch : 41 [1.0287522, 1.0870833]\n",
      "Epoch : 42 [0.9981772, 0.99453485]\n",
      "Epoch : 43 [0.9682136, 0.9056512]\n",
      "Epoch : 44 [0.93884933, 0.8202874]\n",
      "Epoch : 45 [0.9100723, 0.73830396]\n",
      "Epoch : 46 [0.88187087, 0.6595671]\n",
      "Epoch : 47 [0.85423344, 0.5839482]\n",
      "Epoch : 48 [0.8271488, 0.5113239]\n",
      "Epoch : 49 [0.80060583, 0.44157553]\n",
      "Epoch : 50 [0.7745937, 0.37458915]\n",
      "Epoch : 51 [0.7491018, 0.31025535]\n",
      "Epoch : 52 [0.7241198, 0.24846923]\n",
      "Epoch : 53 [0.6996374, 0.18912995]\n",
      "Epoch : 54 [0.67564464, 0.13214028]\n",
      "Epoch : 55 [0.65213174, 0.07740754]\n",
      "Epoch : 56 [0.6290891, 0.024842262]\n",
      "Epoch : 57 [0.60650736, -0.025641501]\n",
      "Epoch : 58 [0.5843772, -0.074126065]\n",
      "Epoch : 59 [0.56268966, -0.1206907]\n",
      "Epoch : 60 [0.5414359, -0.1654113]\n",
      "Epoch : 61 [0.5206072, -0.20836097]\n",
      "Epoch : 62 [0.500195, -0.24960995]\n",
      "Epoch : 63 [0.4801911, -0.2892254]\n",
      "Epoch : 64 [0.4605873, -0.32727203]\n",
      "Epoch : 65 [0.44137555, -0.3638121]\n",
      "Epoch : 66 [0.42254806, -0.3989051]\n",
      "Epoch : 67 [0.40409708, -0.4326085]\n",
      "Epoch : 68 [0.38601515, -0.46497717]\n",
      "Epoch : 69 [0.36829484, -0.49606407]\n",
      "Epoch : 70 [0.35092893, -0.5259199]\n",
      "Epoch : 71 [0.33391035, -0.55459356]\n",
      "Epoch : 72 [0.31723213, -0.5821316]\n",
      "Epoch : 73 [0.3008875, -0.6085793]\n",
      "Epoch : 74 [0.28486976, -0.63397944]\n",
      "Epoch : 75 [0.26917237, -0.6583739]\n",
      "Epoch : 76 [0.25378892, -0.6818023]\n",
      "Epoch : 77 [0.23871315, -0.7043029]\n",
      "Epoch : 78 [0.22393888, -0.7259125]\n",
      "Epoch : 79 [0.20946011, -0.7466663]\n",
      "Epoch : 80 [0.19527091, -0.76659834]\n",
      "Epoch : 81 [0.18136549, -0.7857411]\n",
      "Epoch : 82 [0.16773818, -0.8041257]\n",
      "Epoch : 83 [0.15438342, -0.82178235]\n",
      "Epoch : 84 [0.14129575, -0.83873975]\n",
      "Epoch : 85 [0.12846982, -0.85502565]\n",
      "Epoch : 86 [0.11590043, -0.8706666]\n",
      "Epoch : 87 [0.10358242, -0.88568825]\n",
      "Epoch : 88 [0.09151077, -0.900115]\n",
      "Epoch : 89 [0.079680555, -0.9139705]\n",
      "Epoch : 90 [0.068086945, -0.92727727]\n",
      "Epoch : 91 [0.056725208, -0.94005704]\n",
      "Epoch : 92 [0.045590706, -0.95233077]\n",
      "Epoch : 93 [0.03467889, -0.9641185]\n",
      "Epoch : 94 [0.023985315, -0.97543937]\n",
      "Epoch : 95 [0.013505609, -0.986312]\n",
      "Epoch : 96 [0.0032354966, -0.996754]\n",
      "Epoch : 97 [-0.0068292134, -1.0067827]\n",
      "Epoch : 98 [-0.01669263, -1.0164139]\n",
      "Epoch : 99 [-0.026358776, -1.025664]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.training import gradient_descent\n",
    "\n",
    "x = tf.Variable(3.0, trainable=True)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "@tf.function\n",
    "def f_x():\n",
    "    return x**2 + x - 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch :\", epoch, [x.numpy(), f_x().numpy()])\n",
    "    opt = gradient_descent.GradientDescentOptimizer(0.01).minimize(f_x)\n",
    "    #tf.summary.scalar('loss', f_x().numpy(), step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
