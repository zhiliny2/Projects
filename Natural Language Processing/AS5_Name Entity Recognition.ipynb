{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Class Assignment 5 -- Named Entity Recognition (NER) and Location Extraction\n",
    "\n",
    "Richard Yang"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Identify what is this company name, by looking at the entity distributions across both tweets and news articles\n",
    "2. Identify what other companies are most frequently mentioned along with your primary company\n",
    "    - Analyze what companies are most frequently mentioned within the same document (tweet and news article)\n",
    "    - While analyzing news articles, extract separate entities from titles and texts\n",
    "3. Identify most frequent locations of events, by extracting appropriate named entities\n",
    "    - Locations may include countries, states, cities, regions, etc.\n",
    " \n",
    "\n",
    "In order to complete this analysis:\n",
    "\n",
    "- Discard non-English results\n",
    "- Apply appropriate text cleaning methods\n",
    "- Within your Jupyter notebook:\n",
    "    - Show a table or chart with your top-20 companies (sorted in the descending order)\n",
    "    - You are welcome to use separate tables for titles and texts of the news articles\n",
    "- Use a couple of different NER packages and options, (i.e. both NLTK and SpaCy, also with and without sentence segmentation).  This way you can evaluate which model provided you the best results\n",
    "    - Your top-20 list should only be based on your most accurate results from the best performing NER package\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import nltk as nltk\n",
    "import nltk.corpus  \n",
    "from nltk.text import Text\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample contains 10,012 news articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://kokomoperspective.com/obituaries/jon-w-horton/article_b6ba8e1e-cb9c-11eb-9868-fb11b88b9778.html</td>\n",
       "      <td>2021-06-13</td>\n",
       "      <td>en</td>\n",
       "      <td>Jon W. Horton | Obituaries | kokomoperspective.com</td>\n",
       "      <td>Jon W. Horton | Obituaries | kokomoperspective.comYou have permission to edit this article. EditCloseSign Up                        Log In                    Dashboard  LogoutMy Account Dashboard Profile Saved items LogoutCOVID-19Click here for the latest local news on COVID-19HomeAbout UsContact UsNewsLocalOpinionPoliticsNationalStateAgricultureLifestylesEngagements/Anniversaries/WeddingsAutosEntertainmentHealthHomesOutdoorsSportsNFLNCAAVitalsObituariesAutomotivee-EditionCouponsGalleries74¬∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://auto.economictimes.indiatimes.com/news/auto-components/birla-precision-to-ramp-up-capacity-to-tap-emerging-opportunities-in-india/81254902</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>en</td>\n",
       "      <td>Birla Precision to ramp up capacity to tap emerging opportunities in India, Auto News, ET Auto</td>\n",
       "      <td>Birla Precision to ramp up capacity to tap emerging opportunities in India, Auto News, ET Auto     We have updated our terms and conditions and privacy policy Click \"Continue\" to accept and continue with ET AutoAccept the updated privacy &amp; cookie policyDear user, ET Auto privacy and cookie policy has been updated to align with the new data regulations in European Union. Please review and accept these changes below to continue using the website.You can see our privacy policy &amp; our cookie ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                  url  \\\n",
       "0                                              http://kokomoperspective.com/obituaries/jon-w-horton/article_b6ba8e1e-cb9c-11eb-9868-fb11b88b9778.html   \n",
       "1  https://auto.economictimes.indiatimes.com/news/auto-components/birla-precision-to-ramp-up-capacity-to-tap-emerging-opportunities-in-india/81254902   \n",
       "\n",
       "        date language  \\\n",
       "0 2021-06-13       en   \n",
       "1 2021-02-28       en   \n",
       "\n",
       "                                                                                                 title  \\\n",
       "0                                                   Jon W. Horton | Obituaries | kokomoperspective.com   \n",
       "1       Birla Precision to ramp up capacity to tap emerging opportunities in India, Auto News, ET Auto   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \n",
       "0  Jon W. Horton | Obituaries | kokomoperspective.comYou have permission to edit this article. EditCloseSign Up                        Log In                    Dashboard  LogoutMy Account Dashboard Profile Saved items LogoutCOVID-19Click here for the latest local news on COVID-19HomeAbout UsContact UsNewsLocalOpinionPoliticsNationalStateAgricultureLifestylesEngagements/Anniversaries/WeddingsAutosEntertainmentHealthHomesOutdoorsSportsNFLNCAAVitalsObituariesAutomotivee-EditionCouponsGalleries74¬∞...  \n",
       "1      Birla Precision to ramp up capacity to tap emerging opportunities in India, Auto News, ET Auto     We have updated our terms and conditions and privacy policy Click \"Continue\" to accept and continue with ET AutoAccept the updated privacy & cookie policyDear user, ET Auto privacy and cookie policy has been updated to align with the new data regulations in European Union. Please review and accept these changes below to continue using the website.You can see our privacy policy & our cookie ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_path = 'https://storage.googleapis.com/msca-bdp-data-open/news/nlp_a_5_news.json'\n",
    "news_df = pd.read_json(news_path, orient='records', lines=True)\n",
    "\n",
    "print(f'Sample contains {news_df.shape[0]:,.0f} news articles')\n",
    "news_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10012, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample contains 10,105 tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1534565117614084096</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-06-08</td>\n",
       "      <td>Low Orbit Tourist üåçüì∑</td>\n",
       "      <td></td>\n",
       "      <td>Body &amp;amp; Assembly - Halewood - United Kingdom\\nüåç53.3504,-2.8352296,402m\\n\\nHalewood Body &amp;amp; Assembly is a Jaguar Land Rover factory in Halewood, England, and forms the major part of the Halewood complex which is shared with Ford who manufacture transmissions at the site. [Wikipedia] https://t.co/LPmCnZIaVt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1534565743429394439</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-06-08</td>\n",
       "      <td>CompleteCar.ie</td>\n",
       "      <td>RT</td>\n",
       "      <td>Land Rover Ireland has announced that the new Range Rover Sport starts at ‚Ç¨114,150, now on @completecar:\\n\\nhttps://t.co/TjGUkL3FYr https://t.co/QdVaEiJkjO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id lang       date                  name retweeted  \\\n",
       "0  1534565117614084096   en 2022-06-08  Low Orbit Tourist üåçüì∑             \n",
       "1  1534565743429394439   en 2022-06-08        CompleteCar.ie        RT   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                       text  \n",
       "0  Body &amp; Assembly - Halewood - United Kingdom\\nüåç53.3504,-2.8352296,402m\\n\\nHalewood Body &amp; Assembly is a Jaguar Land Rover factory in Halewood, England, and forms the major part of the Halewood complex which is shared with Ford who manufacture transmissions at the site. [Wikipedia] https://t.co/LPmCnZIaVt  \n",
       "1                                                                                                                                                               Land Rover Ireland has announced that the new Range Rover Sport starts at ‚Ç¨114,150, now on @completecar:\\n\\nhttps://t.co/TjGUkL3FYr https://t.co/QdVaEiJkjO  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_path = 'https://storage.googleapis.com/msca-bdp-data-open/tweets/nlp_a_5_tweets.json'\n",
    "tweets_df = pd.read_json(tweets_path, orient='records', lines=True)\n",
    "print(f'Sample contains {tweets_df.shape[0]:,.0f} tweets')\n",
    "tweets_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Name Entity Recognition (NER) and Location Extraction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I will use Spacy(With or without sentence segmentation) and NLTK(With or without sentence segmentation) to extract the company name and location from tweets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets_Spacy_Without_Sentence_Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check if a token is in English\n",
    "def is_english(token):\n",
    "    return token.lang_ == 'en'\n",
    "\n",
    "# apply the language model to each tweet and filter out non-English tokens\n",
    "def filter_non_english(text):\n",
    "    doc = nlp(text)\n",
    "    english_tokens = [token.text for token in doc if is_english(token)]\n",
    "    # remove single-character tokens\n",
    "    english_tokens = [token for token in english_tokens if len(token) > 1]\n",
    "    # remove urls\n",
    "    english_tokens = [token for token in english_tokens if not token.startswith('http')]\n",
    "    return ' '.join(english_tokens)\n",
    "# apply the function to the 'text' column and store the result in a new column 'text_english'\n",
    "tweets_df['text_english'] = tweets_df['text'].apply(filter_non_english)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract entities from the 'text_english' column\n",
    "entities = []\n",
    "labels = []\n",
    "position_start = []\n",
    "position_end = []\n",
    "\n",
    "for text in tweets_df['text_english']:\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent.text)\n",
    "        labels.append(ent.label_)\n",
    "        position_start.append(ent.start_char)\n",
    "        position_end.append(ent.end_char)\n",
    "\n",
    "# create a dataframe of entities\n",
    "df = pd.DataFrame({'Entities':entities,'Labels':labels,'Position_Start':position_start, 'Position_End':position_end})\n",
    "\n",
    "# count the number of rows where column 'Labels' = 'ORG'\n",
    "df_org = df[df['Labels'] == 'ORG']\n",
    "df_org_loc = df[(df['Labels'] == 'LOC') | (df['Labels'] == 'GPE')]\n",
    "\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_org dataframe, and sort the values in descending order\n",
    "tweets_spacy_noss = pd.DataFrame(df_org['Entities'].value_counts().sort_values(ascending=False).head(20))\n",
    "tweets_spacy_noss_loc = pd.DataFrame(df_org_loc['Entities'].value_counts().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets_Spacy_With_Sentence_Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "labels = []\n",
    "position_start = []\n",
    "position_end = []\n",
    "sentences = []\n",
    "\n",
    "for text in tweets_df['text_english']:\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        for ent in sent.ents:\n",
    "            entities.append(ent.text)\n",
    "            labels.append(ent.label_)\n",
    "            position_start.append(ent.start_char)\n",
    "            position_end.append(ent.end_char)\n",
    "            sentences.append(sent.text)\n",
    "\n",
    "# create a dataframe of entities\n",
    "df = pd.DataFrame({'Entities':entities,'Labels':labels,'Position_Start':position_start, 'Position_End':position_end, 'Sentence':sentences})\n",
    "\n",
    "# count the number of rows where column 'Labels' = 'ORG'\n",
    "\n",
    "df_org = df[df['Labels'] == 'ORG']\n",
    "df_org_loc = df[(df['Labels'] == 'LOC') | (df['Labels'] == 'GPE')]\n",
    "\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_org dataframe, and sort the values in descending order\n",
    "tweets_spacy_ss = pd.DataFrame(df_org['Entities'].value_counts().sort_values(ascending=False).head(20))\n",
    "tweets_spacy_ss_loc = pd.DataFrame(df_org_loc['Entities'].value_counts().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets_NLTK_Witout_Sentence_Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "labels = []\n",
    "for text in tweets_df['text_english']:\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)), binary = False):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            entities.append(' '.join(c[0] for c in chunk)) #Add space as between multi-token entities\n",
    "            labels.append(chunk.label())\n",
    "\n",
    "entities_labels = list(zip(entities, labels)) # zip the two lists together\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\", \"Labels\"]\n",
    "# count the number of rows where column 'Labels' = 'ORG'\n",
    "df_org = entities_df[entities_df['Labels'] == 'ORGANIZATION']\n",
    "df_gpe_loc_NLTK = entities_df[(entities_df['Labels'] == 'GPE')]\n",
    "\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_org dataframe, and sort the values in descending order\n",
    "tweets_nltk_noss = pd.DataFrame(df_org['Entities'].value_counts().sort_values(ascending=False).head(20))\n",
    "tweets_nltk_noss_loc = pd.DataFrame(df_gpe_loc_NLTK['Entities'].value_counts().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets_NLTK_With_Sentence_Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows where column 'Labels' = 'ORGANIZATION'\n",
    "entities = []\n",
    "labels = []\n",
    "\n",
    "for text in tweets_df['text_english']:\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)), binary = False):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                entities.append(' '.join(c[0] for c in chunk)) #Add space as between multi-token entities\n",
    "                labels.append(chunk.label())\n",
    "\n",
    "entities_labels = list(zip(entities, labels)) # zip the two lists together\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\", \"Labels\"]\n",
    "\n",
    "# count the number of rows\n",
    "df_org = entities_df[entities_df['Labels'] == 'ORGANIZATION']\n",
    "df_gpe_loc_NLTK = entities_df[(entities_df['Labels'] == 'GPE')]\n",
    "\n",
    "# get the frequency of each unique entity\n",
    "tweets_NLTK_ss = pd.DataFrame(df_org['Entities'].value_counts().sort_values(ascending=False).head(20))\n",
    "tweets_NLTK_ss_loc = pd.DataFrame(df_gpe_loc_NLTK['Entities'].value_counts().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison for Tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities_Spacy_SS</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Entities_Spacy_NOSS</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Entities_NLTK_SS</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Entities_NLTK_NOSS</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Land Rover</td>\n",
       "      <td>1196</td>\n",
       "      <td>Land Rover</td>\n",
       "      <td>1196</td>\n",
       "      <td>Land Rover</td>\n",
       "      <td>995</td>\n",
       "      <td>Land Rover</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jaguar Land Rover</td>\n",
       "      <td>647</td>\n",
       "      <td>Jaguar Land Rover</td>\n",
       "      <td>647</td>\n",
       "      <td>Land</td>\n",
       "      <td>521</td>\n",
       "      <td>Land</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eBay</td>\n",
       "      <td>409</td>\n",
       "      <td>eBay</td>\n",
       "      <td>409</td>\n",
       "      <td>General Motors</td>\n",
       "      <td>283</td>\n",
       "      <td>General Motors</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General Motors</td>\n",
       "      <td>284</td>\n",
       "      <td>General Motors</td>\n",
       "      <td>284</td>\n",
       "      <td>LAND</td>\n",
       "      <td>227</td>\n",
       "      <td>LAND</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jaguar Land Rover BMW Mercedes Benz Citroen</td>\n",
       "      <td>241</td>\n",
       "      <td>Jaguar Land Rover BMW Mercedes Benz Citroen</td>\n",
       "      <td>241</td>\n",
       "      <td>eBay</td>\n",
       "      <td>202</td>\n",
       "      <td>eBay</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ford</td>\n",
       "      <td>104</td>\n",
       "      <td>Ford</td>\n",
       "      <td>104</td>\n",
       "      <td>NigelAndArron</td>\n",
       "      <td>190</td>\n",
       "      <td>NigelAndArron</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Volvo</td>\n",
       "      <td>87</td>\n",
       "      <td>Volvo</td>\n",
       "      <td>87</td>\n",
       "      <td>SUV</td>\n",
       "      <td>182</td>\n",
       "      <td>SUV</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jaguar</td>\n",
       "      <td>76</td>\n",
       "      <td>Jaguar</td>\n",
       "      <td>76</td>\n",
       "      <td>Duke</td>\n",
       "      <td>178</td>\n",
       "      <td>Duke</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BMW</td>\n",
       "      <td>72</td>\n",
       "      <td>BMW</td>\n",
       "      <td>72</td>\n",
       "      <td>Jaguar Land Rover</td>\n",
       "      <td>163</td>\n",
       "      <td>Jaguar Land Rover</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>n‚Äôt</td>\n",
       "      <td>69</td>\n",
       "      <td>n‚Äôt</td>\n",
       "      <td>69</td>\n",
       "      <td>UK</td>\n",
       "      <td>156</td>\n",
       "      <td>UK</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the Jaguar Land Rover Driving Challenge</td>\n",
       "      <td>66</td>\n",
       "      <td>the Jaguar Land Rover Driving Challenge</td>\n",
       "      <td>66</td>\n",
       "      <td>Duchess</td>\n",
       "      <td>148</td>\n",
       "      <td>Duchess</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SHAMELESS</td>\n",
       "      <td>64</td>\n",
       "      <td>SHAMELESS</td>\n",
       "      <td>64</td>\n",
       "      <td>Land Rover Discovery</td>\n",
       "      <td>142</td>\n",
       "      <td>Land Rover Discovery</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>the SHAMELESS Health Services Board of Zimbabwe</td>\n",
       "      <td>64</td>\n",
       "      <td>the SHAMELESS Health Services Board of Zimbabwe</td>\n",
       "      <td>64</td>\n",
       "      <td>SHAMELESS</td>\n",
       "      <td>93</td>\n",
       "      <td>SHAMELESS</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>UKmfg</td>\n",
       "      <td>62</td>\n",
       "      <td>UKmfg</td>\n",
       "      <td>62</td>\n",
       "      <td>ROVER</td>\n",
       "      <td>92</td>\n",
       "      <td>ROVER</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tata Motors</td>\n",
       "      <td>59</td>\n",
       "      <td>Tata Motors</td>\n",
       "      <td>59</td>\n",
       "      <td>Rover</td>\n",
       "      <td>92</td>\n",
       "      <td>Rover</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Tata</td>\n",
       "      <td>57</td>\n",
       "      <td>Tata</td>\n",
       "      <td>57</td>\n",
       "      <td>Queen</td>\n",
       "      <td>83</td>\n",
       "      <td>Queen</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>56</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>56</td>\n",
       "      <td>Jaguar Land</td>\n",
       "      <td>80</td>\n",
       "      <td>Jaguar Land</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tesla Bentley Jaguar Jeep Land Rover</td>\n",
       "      <td>51</td>\n",
       "      <td>Tesla Bentley Jaguar Jeep Land Rover</td>\n",
       "      <td>51</td>\n",
       "      <td>BaT</td>\n",
       "      <td>77</td>\n",
       "      <td>BaT</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Grenadier</td>\n",
       "      <td>51</td>\n",
       "      <td>Grenadier</td>\n",
       "      <td>51</td>\n",
       "      <td>InvictusGames</td>\n",
       "      <td>74</td>\n",
       "      <td>InvictusGames</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Rover</td>\n",
       "      <td>51</td>\n",
       "      <td>Rover</td>\n",
       "      <td>51</td>\n",
       "      <td>SHAMELESS Health Services Board</td>\n",
       "      <td>64</td>\n",
       "      <td>SHAMELESS Health Services Board</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Entities_Spacy_SS  Frequency  \\\n",
       "0                                        Land Rover       1196   \n",
       "1                                 Jaguar Land Rover        647   \n",
       "2                                              eBay        409   \n",
       "3                                    General Motors        284   \n",
       "4       Jaguar Land Rover BMW Mercedes Benz Citroen        241   \n",
       "5                                              Ford        104   \n",
       "6                                             Volvo         87   \n",
       "7                                            Jaguar         76   \n",
       "8                                               BMW         72   \n",
       "9                                               n‚Äôt         69   \n",
       "10          the Jaguar Land Rover Driving Challenge         66   \n",
       "11                                        SHAMELESS         64   \n",
       "12  the SHAMELESS Health Services Board of Zimbabwe         64   \n",
       "13                                            UKmfg         62   \n",
       "14                                      Tata Motors         59   \n",
       "15                                             Tata         57   \n",
       "16                                           Toyota         56   \n",
       "17             Tesla Bentley Jaguar Jeep Land Rover         51   \n",
       "18                                        Grenadier         51   \n",
       "19                                            Rover         51   \n",
       "\n",
       "                                Entities_Spacy_NOSS  Frequency  \\\n",
       "0                                        Land Rover       1196   \n",
       "1                                 Jaguar Land Rover        647   \n",
       "2                                              eBay        409   \n",
       "3                                    General Motors        284   \n",
       "4       Jaguar Land Rover BMW Mercedes Benz Citroen        241   \n",
       "5                                              Ford        104   \n",
       "6                                             Volvo         87   \n",
       "7                                            Jaguar         76   \n",
       "8                                               BMW         72   \n",
       "9                                               n‚Äôt         69   \n",
       "10          the Jaguar Land Rover Driving Challenge         66   \n",
       "11                                        SHAMELESS         64   \n",
       "12  the SHAMELESS Health Services Board of Zimbabwe         64   \n",
       "13                                            UKmfg         62   \n",
       "14                                      Tata Motors         59   \n",
       "15                                             Tata         57   \n",
       "16                                           Toyota         56   \n",
       "17             Tesla Bentley Jaguar Jeep Land Rover         51   \n",
       "18                                        Grenadier         51   \n",
       "19                                            Rover         51   \n",
       "\n",
       "                   Entities_NLTK_SS  Frequency  \\\n",
       "0                        Land Rover        995   \n",
       "1                              Land        521   \n",
       "2                    General Motors        283   \n",
       "3                              LAND        227   \n",
       "4                              eBay        202   \n",
       "5                     NigelAndArron        190   \n",
       "6                               SUV        182   \n",
       "7                              Duke        178   \n",
       "8                 Jaguar Land Rover        163   \n",
       "9                                UK        156   \n",
       "10                          Duchess        148   \n",
       "11             Land Rover Discovery        142   \n",
       "12                        SHAMELESS         93   \n",
       "13                            ROVER         92   \n",
       "14                            Rover         92   \n",
       "15                            Queen         83   \n",
       "16                      Jaguar Land         80   \n",
       "17                              BaT         77   \n",
       "18                    InvictusGames         74   \n",
       "19  SHAMELESS Health Services Board         64   \n",
       "\n",
       "                 Entities_NLTK_NOSS  Frequency  \n",
       "0                        Land Rover        995  \n",
       "1                              Land        521  \n",
       "2                    General Motors        283  \n",
       "3                              LAND        227  \n",
       "4                              eBay        202  \n",
       "5                     NigelAndArron        190  \n",
       "6                               SUV        182  \n",
       "7                              Duke        178  \n",
       "8                 Jaguar Land Rover        163  \n",
       "9                                UK        156  \n",
       "10                          Duchess        148  \n",
       "11             Land Rover Discovery        142  \n",
       "12                        SHAMELESS         93  \n",
       "13                            ROVER         92  \n",
       "14                            Rover         92  \n",
       "15                            Queen         83  \n",
       "16                      Jaguar Land         80  \n",
       "17                              BaT         77  \n",
       "18                    InvictusGames         74  \n",
       "19  SHAMELESS Health Services Board         64  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result Comparison for NER\n",
    "tweets_NLTK_ss_show = tweets_NLTK_ss.reset_index()\n",
    "tweets_NLTK_ss_show.columns = ['Entities_NLTK_SS', 'Frequency']\n",
    "tweets_nltk_noss_show = tweets_nltk_noss.reset_index()\n",
    "tweets_nltk_noss_show.columns = ['Entities_NLTK_NOSS', 'Frequency']\n",
    "tweets_spacy_ss_show = tweets_spacy_ss.reset_index()\n",
    "tweets_spacy_ss_show.columns = ['Entities_Spacy_SS', 'Frequency']\n",
    "tweets_spacy_noss_show = tweets_spacy_noss.reset_index()\n",
    "tweets_spacy_noss_show.columns = ['Entities_Spacy_NOSS', 'Frequency']\n",
    "\n",
    "# concatenate the dataframes\n",
    "tweets_ner_show = pd.concat([tweets_spacy_ss_show, tweets_spacy_noss_show, tweets_NLTK_ss_show, tweets_nltk_noss_show], axis=1)\n",
    "tweets_ner_show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities_Spacy_SS</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Entities_Spacy_NOSS</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Entities_NLTK_SS</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Entities_NLTK_NOSS</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russia</td>\n",
       "      <td>471</td>\n",
       "      <td>Russia</td>\n",
       "      <td>471</td>\n",
       "      <td>Land</td>\n",
       "      <td>1553</td>\n",
       "      <td>Land</td>\n",
       "      <td>1553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK</td>\n",
       "      <td>365</td>\n",
       "      <td>UK</td>\n",
       "      <td>365</td>\n",
       "      <td>Russia</td>\n",
       "      <td>464</td>\n",
       "      <td>Russia</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n‚Äôt</td>\n",
       "      <td>208</td>\n",
       "      <td>n‚Äôt</td>\n",
       "      <td>208</td>\n",
       "      <td>British</td>\n",
       "      <td>188</td>\n",
       "      <td>British</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>88</td>\n",
       "      <td>India</td>\n",
       "      <td>88</td>\n",
       "      <td>Sussex</td>\n",
       "      <td>119</td>\n",
       "      <td>Sussex</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kibaki</td>\n",
       "      <td>76</td>\n",
       "      <td>Kibaki</td>\n",
       "      <td>76</td>\n",
       "      <td>India</td>\n",
       "      <td>87</td>\n",
       "      <td>India</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Meghan</td>\n",
       "      <td>72</td>\n",
       "      <td>Meghan</td>\n",
       "      <td>72</td>\n",
       "      <td>Russian</td>\n",
       "      <td>84</td>\n",
       "      <td>Russian</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jamaica</td>\n",
       "      <td>66</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>66</td>\n",
       "      <td>New</td>\n",
       "      <td>79</td>\n",
       "      <td>New</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Britain</td>\n",
       "      <td>60</td>\n",
       "      <td>Britain</td>\n",
       "      <td>60</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>75</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>41</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>41</td>\n",
       "      <td>LAND</td>\n",
       "      <td>71</td>\n",
       "      <td>LAND</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>London</td>\n",
       "      <td>39</td>\n",
       "      <td>London</td>\n",
       "      <td>39</td>\n",
       "      <td>Car</td>\n",
       "      <td>68</td>\n",
       "      <td>Car</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>35</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>35</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>68</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>China</td>\n",
       "      <td>33</td>\n",
       "      <td>China</td>\n",
       "      <td>33</td>\n",
       "      <td>Paracetamol</td>\n",
       "      <td>64</td>\n",
       "      <td>Paracetamol</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Snatch Land Rover</td>\n",
       "      <td>29</td>\n",
       "      <td>Snatch Land Rover</td>\n",
       "      <td>29</td>\n",
       "      <td>Hagley</td>\n",
       "      <td>62</td>\n",
       "      <td>Hagley</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Hague</td>\n",
       "      <td>29</td>\n",
       "      <td>The Hague</td>\n",
       "      <td>29</td>\n",
       "      <td>Meghan</td>\n",
       "      <td>55</td>\n",
       "      <td>Meghan</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MeghanMarkle</td>\n",
       "      <td>28</td>\n",
       "      <td>MeghanMarkle</td>\n",
       "      <td>28</td>\n",
       "      <td>Indian</td>\n",
       "      <td>53</td>\n",
       "      <td>Indian</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>25</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>25</td>\n",
       "      <td>Prince</td>\n",
       "      <td>51</td>\n",
       "      <td>Prince</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>France</td>\n",
       "      <td>25</td>\n",
       "      <td>France</td>\n",
       "      <td>25</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>50</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>24</td>\n",
       "      <td>Hollywood</td>\n",
       "      <td>24</td>\n",
       "      <td>Kenyan</td>\n",
       "      <td>46</td>\n",
       "      <td>Kenyan</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Nyeri</td>\n",
       "      <td>23</td>\n",
       "      <td>Nyeri</td>\n",
       "      <td>23</td>\n",
       "      <td>Meet</td>\n",
       "      <td>39</td>\n",
       "      <td>Meet</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ukraine</td>\n",
       "      <td>22</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>22</td>\n",
       "      <td>TEKNO</td>\n",
       "      <td>38</td>\n",
       "      <td>TEKNO</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entities_Spacy_SS  Frequency Entities_Spacy_NOSS  Frequency  \\\n",
       "0              Russia        471              Russia        471   \n",
       "1                  UK        365                  UK        365   \n",
       "2                 n‚Äôt        208                 n‚Äôt        208   \n",
       "3               India         88               India         88   \n",
       "4              Kibaki         76              Kibaki         76   \n",
       "5              Meghan         72              Meghan         72   \n",
       "6             Jamaica         66             Jamaica         66   \n",
       "7             Britain         60             Britain         60   \n",
       "8            Zimbabwe         41            Zimbabwe         41   \n",
       "9              London         39              London         39   \n",
       "10       South Africa         35        South Africa         35   \n",
       "11              China         33               China         33   \n",
       "12  Snatch Land Rover         29   Snatch Land Rover         29   \n",
       "13          The Hague         29           The Hague         29   \n",
       "14       MeghanMarkle         28        MeghanMarkle         28   \n",
       "15        New Zealand         25         New Zealand         25   \n",
       "16             France         25              France         25   \n",
       "17          Hollywood         24           Hollywood         24   \n",
       "18              Nyeri         23               Nyeri         23   \n",
       "19            Ukraine         22             Ukraine         22   \n",
       "\n",
       "   Entities_NLTK_SS  Frequency Entities_NLTK_NOSS  Frequency  \n",
       "0              Land       1553               Land       1553  \n",
       "1            Russia        464             Russia        464  \n",
       "2           British        188            British        188  \n",
       "3            Sussex        119             Sussex        119  \n",
       "4             India         87              India         87  \n",
       "5           Russian         84            Russian         84  \n",
       "6               New         79                New         79  \n",
       "7          Zimbabwe         75           Zimbabwe         75  \n",
       "8              LAND         71               LAND         71  \n",
       "9               Car         68                Car         68  \n",
       "10        Cambridge         68          Cambridge         68  \n",
       "11      Paracetamol         64        Paracetamol         64  \n",
       "12           Hagley         62             Hagley         62  \n",
       "13           Meghan         55             Meghan         55  \n",
       "14           Indian         53             Indian         53  \n",
       "15           Prince         51             Prince         51  \n",
       "16          Jamaica         50            Jamaica         50  \n",
       "17           Kenyan         46             Kenyan         46  \n",
       "18             Meet         39               Meet         39  \n",
       "19            TEKNO         38              TEKNO         38  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result Comparison for Location\n",
    "tweets_NLTK_ss_loc_show = tweets_NLTK_ss_loc.reset_index()\n",
    "tweets_NLTK_ss_loc_show.columns = ['Entities_NLTK_SS', 'Frequency']\n",
    "tweets_nltk_noss_loc_show = tweets_nltk_noss_loc.reset_index()\n",
    "tweets_nltk_noss_loc_show.columns = ['Entities_NLTK_NOSS', 'Frequency']\n",
    "tweets_spacy_ss_loc_show = tweets_spacy_ss_loc.reset_index()\n",
    "tweets_spacy_ss_loc_show.columns = ['Entities_Spacy_SS', 'Frequency']\n",
    "tweets_spacy_noss_loc_show = tweets_spacy_noss_loc.reset_index()\n",
    "tweets_spacy_noss_loc_show.columns = ['Entities_Spacy_NOSS', 'Frequency']\n",
    "\n",
    "# concatenate the dataframes\n",
    "tweets_ner_loc_show = pd.concat([tweets_spacy_ss_loc_show, tweets_spacy_noss_loc_show, tweets_NLTK_ss_loc_show, tweets_nltk_noss_loc_show], axis=1)\n",
    "tweets_ner_loc_show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Title Name Entity Recognition (NER) and Location Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For News Title, it is not necessary to do sentence segmentation as the title is already a sentence.Therefore, I will only use Spacy and NLTK(Without sentence segmentation) to extract the company name and location from news title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check if a token is in English\n",
    "def is_english(token):\n",
    "    return token.lang_ == 'en'\n",
    "\n",
    "# apply the language model to each tweet and filter out non-English tokens\n",
    "def clean_text(text):\n",
    "    doc = nlp(text)\n",
    "    english_tokens = [token.text for token in doc if is_english(token)]\n",
    "    # remove single-character tokens\n",
    "    english_tokens = [token for token in english_tokens if len(token) > 1]\n",
    "    # remove urls\n",
    "    english_tokens = [token for token in english_tokens if not token.startswith('http')]\n",
    "    return ' '.join(english_tokens)\n",
    "# apply the function to the 'text' column and store the result in a new column 'text_english'\n",
    "news_df['title_new'] = news_df['title'].apply(clean_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News_Title_Spacy_Without_Sentence_Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract entities from the 'text_english' column\n",
    "entities = []\n",
    "labels = []\n",
    "position_start = []\n",
    "position_end = []\n",
    "\n",
    "for text in news_df['title']:\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent.text)\n",
    "        labels.append(ent.label_)\n",
    "        position_start.append(ent.start_char)\n",
    "        position_end.append(ent.end_char)\n",
    "\n",
    "# create a dataframe of entities\n",
    "df = pd.DataFrame({'Entities':entities,'Labels':labels,'Position_Start':position_start, 'Position_End':position_end})\n",
    "\n",
    "# count the number of rows where column 'Labels' = 'ORG'\n",
    "\n",
    "df_org = df[df['Labels'] == 'ORG']\n",
    "df_org_loc = df[(df['Labels'] == 'LOC') | (df['Labels'] == 'GPE')]\n",
    "\n",
    "\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_org dataframe, and sort the values in descending order\n",
    "news_title_spacy_noss = pd.DataFrame(df_org['Entities'].value_counts().sort_values(ascending=False).head(20))\n",
    "news_title_spacy_noss_loc = pd.DataFrame(df_org_loc['Entities'].value_counts().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News_Title_NLTK_Without_Sentence_Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "labels = []\n",
    "for text in tweets_df['text_english']:\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)), binary = False):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            entities.append(' '.join(c[0] for c in chunk)) #Add space as between multi-token entities\n",
    "            labels.append(chunk.label())\n",
    "\n",
    "entities_labels = list(zip(entities, labels)) # zip the two lists together\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\", \"Labels\"]\n",
    "# count the number of rows where column 'Labels' = 'ORG'\n",
    "df_org = entities_df[entities_df['Labels'] == 'ORGANIZATION']\n",
    "df_gpe_loc_NLTK = entities_df[(entities_df['Labels'] == 'GPE')]\n",
    "\n",
    "\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_org dataframe, and sort the values in descending order\n",
    "news_title_nltk_noss = pd.DataFrame(df_org['Entities'].value_counts().sort_values(ascending=False).head(20))\n",
    "news_title_nltk_noss_loc = pd.DataFrame(df_gpe_loc_NLTK['Entities'].value_counts().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Comparison for News Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities_Spacy_NOSS</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Entities_NLTK_NOSS</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ford</td>\n",
       "      <td>270</td>\n",
       "      <td>Land Rover</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daily Mail Online</td>\n",
       "      <td>212</td>\n",
       "      <td>Land</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Star News</td>\n",
       "      <td>209</td>\n",
       "      <td>General Motors</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hyundai</td>\n",
       "      <td>205</td>\n",
       "      <td>LAND</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>162</td>\n",
       "      <td>eBay</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>160</td>\n",
       "      <td>NigelAndArron</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honda</td>\n",
       "      <td>146</td>\n",
       "      <td>SUV</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shropshire Star</td>\n",
       "      <td>126</td>\n",
       "      <td>Duke</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Express &amp; Star</td>\n",
       "      <td>120</td>\n",
       "      <td>Jaguar Land Rover</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Automotive News</td>\n",
       "      <td>108</td>\n",
       "      <td>UK</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BMW</td>\n",
       "      <td>107</td>\n",
       "      <td>Duchess</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RAM</td>\n",
       "      <td>90</td>\n",
       "      <td>Land Rover Discovery</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Otago Daily Times Online News</td>\n",
       "      <td>89</td>\n",
       "      <td>SHAMELESS</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ET Auto</td>\n",
       "      <td>76</td>\n",
       "      <td>ROVER</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Auto News</td>\n",
       "      <td>74</td>\n",
       "      <td>Rover</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EV</td>\n",
       "      <td>72</td>\n",
       "      <td>Queen</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Technology News</td>\n",
       "      <td>71</td>\n",
       "      <td>Jaguar Land</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Jeep</td>\n",
       "      <td>68</td>\n",
       "      <td>BaT</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>67</td>\n",
       "      <td>InvictusGames</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>54</td>\n",
       "      <td>SHAMELESS Health Services Board</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Entities_Spacy_NOSS  Frequency               Entities_NLTK_NOSS  \\\n",
       "0                            Ford        270                       Land Rover   \n",
       "1               Daily Mail Online        212                             Land   \n",
       "2                       Star News        209                   General Motors   \n",
       "3                         Hyundai        205                             LAND   \n",
       "4                          Toyota        162                             eBay   \n",
       "5                       Chevrolet        160                    NigelAndArron   \n",
       "6                           Honda        146                              SUV   \n",
       "7                 Shropshire Star        126                             Duke   \n",
       "8                  Express & Star        120                Jaguar Land Rover   \n",
       "9                 Automotive News        108                               UK   \n",
       "10                            BMW        107                          Duchess   \n",
       "11                            RAM         90             Land Rover Discovery   \n",
       "12  Otago Daily Times Online News         89                        SHAMELESS   \n",
       "13                        ET Auto         76                            ROVER   \n",
       "14                      Auto News         74                            Rover   \n",
       "15                             EV         72                            Queen   \n",
       "16                Technology News         71                      Jaguar Land   \n",
       "17                           Jeep         68                              BaT   \n",
       "18                         Nissan         67                    InvictusGames   \n",
       "19                     Volkswagen         54  SHAMELESS Health Services Board   \n",
       "\n",
       "    Frequency  \n",
       "0         995  \n",
       "1         521  \n",
       "2         283  \n",
       "3         227  \n",
       "4         202  \n",
       "5         190  \n",
       "6         182  \n",
       "7         178  \n",
       "8         163  \n",
       "9         156  \n",
       "10        148  \n",
       "11        142  \n",
       "12         93  \n",
       "13         92  \n",
       "14         92  \n",
       "15         83  \n",
       "16         80  \n",
       "17         77  \n",
       "18         74  \n",
       "19         64  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result Comparison for News Title NER\n",
    "news_title_spacy_noss_show = news_title_spacy_noss.reset_index()\n",
    "news_title_spacy_noss_show.columns = ['Entities_Spacy_NOSS', 'Frequency']\n",
    "news_title_nltk_noss_show = news_title_nltk_noss.reset_index()\n",
    "news_title_nltk_noss_show.columns = ['Entities_NLTK_NOSS', 'Frequency']\n",
    "\n",
    "# concatenate the dataframes\n",
    "news_title_ner_show = pd.concat([news_title_spacy_noss_show, news_title_nltk_noss_show], axis=1)\n",
    "news_title_ner_show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For News Title, Ford is the most frequently mentioned company in Spacy. However, in NLTK, it is not the case. The most frequently mentioned company is Land Rover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities_Spacy_NOSS</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Entities_NLTK_NOSS</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carpages.ca</td>\n",
       "      <td>1962</td>\n",
       "      <td>Land</td>\n",
       "      <td>1553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ontario</td>\n",
       "      <td>1316</td>\n",
       "      <td>Russia</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>British Columbia</td>\n",
       "      <td>198</td>\n",
       "      <td>British</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UK</td>\n",
       "      <td>194</td>\n",
       "      <td>Sussex</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Manitoba</td>\n",
       "      <td>181</td>\n",
       "      <td>India</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Winnipeg</td>\n",
       "      <td>137</td>\n",
       "      <td>Russian</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>India</td>\n",
       "      <td>121</td>\n",
       "      <td>New</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Toronto</td>\n",
       "      <td>118</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alberta</td>\n",
       "      <td>116</td>\n",
       "      <td>LAND</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>London</td>\n",
       "      <td>112</td>\n",
       "      <td>Car</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cambridge</td>\n",
       "      <td>90</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>US</td>\n",
       "      <td>84</td>\n",
       "      <td>Paracetamol</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Saskatchewan</td>\n",
       "      <td>78</td>\n",
       "      <td>Hagley</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>North York</td>\n",
       "      <td>72</td>\n",
       "      <td>Meghan</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Calgary</td>\n",
       "      <td>64</td>\n",
       "      <td>Indian</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>64</td>\n",
       "      <td>Prince</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>China</td>\n",
       "      <td>59</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Taiwan</td>\n",
       "      <td>54</td>\n",
       "      <td>Kenyan</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kitchener</td>\n",
       "      <td>45</td>\n",
       "      <td>Meet</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Australia</td>\n",
       "      <td>44</td>\n",
       "      <td>TEKNO</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entities_Spacy_NOSS  Frequency Entities_NLTK_NOSS  Frequency\n",
       "0   Carpages.ca               1962               Land       1553\n",
       "1               Ontario       1316             Russia        464\n",
       "2      British Columbia        198            British        188\n",
       "3                    UK        194             Sussex        119\n",
       "4              Manitoba        181              India         87\n",
       "5              Winnipeg        137            Russian         84\n",
       "6                 India        121                New         79\n",
       "7               Toronto        118           Zimbabwe         75\n",
       "8               Alberta        116               LAND         71\n",
       "9                London        112                Car         68\n",
       "10            Cambridge         90          Cambridge         68\n",
       "11                   US         84        Paracetamol         64\n",
       "12         Saskatchewan         78             Hagley         62\n",
       "13           North York         72             Meghan         55\n",
       "14              Calgary         64             Indian         53\n",
       "15                 U.S.         64             Prince         51\n",
       "16                China         59            Jamaica         50\n",
       "17               Taiwan         54             Kenyan         46\n",
       "18            Kitchener         45               Meet         39\n",
       "19            Australia         44              TEKNO         38"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result Comparison for News Title Location\n",
    "news_title_spacy_noss_loc_show = news_title_spacy_noss_loc.reset_index()\n",
    "news_title_spacy_noss_loc_show.columns = ['Entities_Spacy_NOSS', 'Frequency']\n",
    "news_title_nltk_noss_loc_show = news_title_nltk_noss_loc.reset_index()\n",
    "news_title_nltk_noss_loc_show.columns = ['Entities_NLTK_NOSS', 'Frequency']\n",
    "\n",
    "# concatenate the dataframes\n",
    "news_title_ner_loc_show = pd.concat([news_title_spacy_noss_loc_show, news_title_nltk_noss_loc_show], axis=1)\n",
    "news_title_ner_loc_show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Text Name Entity Recognition (NER) and Location Extraction\n",
    "\n",
    "In this part, Since Spacy without sentence segmentation has the best performance based on the previous results, I will use Spacy without sentence segmentation and NLTK (With or without sentence segmentation) to extract the company name and location from news text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# define a regular expression to match URLs\n",
    "url_pattern = re.compile(r'https?://\\S+')\n",
    "\n",
    "# apply the language model to each tweet, remove URLs, and return the cleaned text\n",
    "clean_text = lambda text: url_pattern.sub('', text)\n",
    "\n",
    "# apply the function to the 'text' column and store the result in a new column 'text_new'\n",
    "news_df['text_new'] = news_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_entities = []\n",
    "index = []\n",
    "labels = []\n",
    "for doc in nlp.pipe(\n",
    "    news_df[\"text_new\"], \n",
    "    disable=[\"tok2vec\",\"tagger\",\"parser\",\"attribute ruler\", \"lemmatizer\"],\n",
    "    batch_size=100,\n",
    "    n_process=2\n",
    "):\n",
    "    for ent in doc.ents:\n",
    "        text_entities.append(ent.text)\n",
    "        labels.append(ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe, count the number of occurrences of each 'text_entities', and sort the values in descending order. The dataframe contains two columns: 'Entities' and 'Count'\n",
    "news_text_spacy_noss_df = pd.DataFrame({'Entities':text_entities,'Labels':labels})\n",
    "\n",
    "# count the number of rows where column 'Labels' = 'ORG'\n",
    "news_text_spacy_noss = news_text_spacy_noss_df[news_text_spacy_noss_df['Labels'] == 'ORG']\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_org dataframe, and sort the values in descending order\n",
    "news_text_spacy_noss = pd.DataFrame(news_text_spacy_noss['Entities'].value_counts().sort_values(ascending=False).head(20))\n",
    "\n",
    "# count the number of rows where column 'Labels' = 'GPE' or 'LOC'\n",
    "news_text_spacy_noss_loc = news_text_spacy_noss_df[(news_text_spacy_noss_df['Labels'] == 'GPE') | (news_text_spacy_noss_df['Labels'] == 'LOC')]\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_org dataframe, and sort the values in descending order\n",
    "news_text_spacy_noss_loc = pd.DataFrame(news_text_spacy_noss_loc['Entities'].value_counts().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK_News_Text without sentence segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since NLTK is too computational expensive, even though I tried parallel processing and multiprocessing, but I found my local machine is not able to run it. Therefore, I have no choince but to sample 2000 news articles to run NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df_sample = news_df.sample(n=2000, random_state=1)\n",
    "news_df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 27min 45s\n",
      "Wall time: 27min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "entities = []\n",
    "labels = []\n",
    "for text in news_df_sample['text_new']:\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)), binary = False):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            entities.append(' '.join(c[0] for c in chunk)) #Add space as between multi-token entities\n",
    "            labels.append(chunk.label())\n",
    "\n",
    "entities_labels = list(zip(entities, labels)) # zip the two lists together\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\", \"Labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count the number of rows where column 'Labels' = 'ORG'\n",
    "df_org = entities_df[entities_df['Labels'] == 'ORGANIZATION']\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_org dataframe, and sort the values in descending order\n",
    "news_text_nltk_noss = pd.DataFrame(df_org['Entities'].value_counts().sort_values(ascending=False).head(20))\n",
    "\n",
    "# create a subset of the dataframe where column 'Labels' = 'GPE' or 'LOCATION' in entities_df\n",
    "df_loc = entities_df[(entities_df['Labels'] == 'GPE') | (entities_df['Labels'] == 'LOCATION')]\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_loc dataframe, and sort the values in descending order\n",
    "news_text_nltk_noss_loc = pd.DataFrame(df_loc['Entities'].value_counts().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK_News_Text with sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 29min 20s\n",
      "Wall time: 29min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# count the number of rows where column 'Labels' = 'ORGANIZATION'\n",
    "entities = []\n",
    "labels = []\n",
    "\n",
    "for text in news_df_sample['text_new']:\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)), binary = False):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                entities.append(' '.join(c[0] for c in chunk)) #Add space as between multi-token entities\n",
    "                labels.append(chunk.label())\n",
    "\n",
    "entities_labels = list(zip(entities, labels)) # zip the two lists together\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\", \"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count the number of rows where column 'Labels' = 'ORG'\n",
    "news_text_nltk_ss = entities_df[entities_df['Labels'] == 'ORGANIZATION']\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_org dataframe, and sort the values in descending order\n",
    "news_text_nltk_ss = pd.DataFrame(news_text_nltk_ss['Entities'].value_counts().sort_values(ascending=False).head(20))\n",
    "# create a subset of the dataframe where column 'Labels' = 'GPE' or 'LOCATION' in entities_df\n",
    "df_loc = entities_df[(entities_df['Labels'] == 'GPE') | (entities_df['Labels'] == 'LOCATION')]\n",
    "# get the frequency of each unique entity in the column 'Entities' in the df_loc dataframe, and sort the values in descending order\n",
    "news_text_nltk_ss_loc = pd.DataFrame(df_loc['Entities'].value_counts().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Comparison for News Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacy_noss</th>\n",
       "      <th>Frequency1</th>\n",
       "      <th>nltk_ss</th>\n",
       "      <th>Frequency2</th>\n",
       "      <th>nltk_noss</th>\n",
       "      <th>Frequency3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MailOnline</td>\n",
       "      <td>8849</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2090</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>6374</td>\n",
       "      <td>MailOnline</td>\n",
       "      <td>1886</td>\n",
       "      <td>MailOnline</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ford</td>\n",
       "      <td>5624</td>\n",
       "      <td>VERY</td>\n",
       "      <td>1576</td>\n",
       "      <td>VERY</td>\n",
       "      <td>1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>5475</td>\n",
       "      <td>Duke</td>\n",
       "      <td>1227</td>\n",
       "      <td>Duke</td>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hyundai</td>\n",
       "      <td>4333</td>\n",
       "      <td>LA</td>\n",
       "      <td>1177</td>\n",
       "      <td>LA</td>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Instagram</td>\n",
       "      <td>4023</td>\n",
       "      <td>Queen</td>\n",
       "      <td>1106</td>\n",
       "      <td>Queen</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Trump</td>\n",
       "      <td>3877</td>\n",
       "      <td>UK</td>\n",
       "      <td>982</td>\n",
       "      <td>UK</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Honda</td>\n",
       "      <td>3786</td>\n",
       "      <td>COVID</td>\n",
       "      <td>945</td>\n",
       "      <td>COVID</td>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BMW</td>\n",
       "      <td>3783</td>\n",
       "      <td>Conditions</td>\n",
       "      <td>881</td>\n",
       "      <td>Conditions</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>3595</td>\n",
       "      <td>THE</td>\n",
       "      <td>738</td>\n",
       "      <td>THE</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Netflix</td>\n",
       "      <td>3406</td>\n",
       "      <td>Princess Diana</td>\n",
       "      <td>729</td>\n",
       "      <td>Princess Diana</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Palace</td>\n",
       "      <td>3214</td>\n",
       "      <td>US</td>\n",
       "      <td>716</td>\n",
       "      <td>Duchess</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>COVID</td>\n",
       "      <td>3140</td>\n",
       "      <td>Duchess</td>\n",
       "      <td>708</td>\n",
       "      <td>US</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EV</td>\n",
       "      <td>2970</td>\n",
       "      <td>Prince Philip</td>\n",
       "      <td>702</td>\n",
       "      <td>Prince Philip</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Britney Spears</td>\n",
       "      <td>2734</td>\n",
       "      <td>LACMA</td>\n",
       "      <td>618</td>\n",
       "      <td>LACMA</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>House</td>\n",
       "      <td>2437</td>\n",
       "      <td>Princess</td>\n",
       "      <td>580</td>\n",
       "      <td>Contact</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BBC</td>\n",
       "      <td>2427</td>\n",
       "      <td>Contact</td>\n",
       "      <td>551</td>\n",
       "      <td>SUV</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>2262</td>\n",
       "      <td>SUV</td>\n",
       "      <td>550</td>\n",
       "      <td>Astroworld</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Duke</td>\n",
       "      <td>2226</td>\n",
       "      <td>House</td>\n",
       "      <td>542</td>\n",
       "      <td>BBC</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>2096</td>\n",
       "      <td>Astroworld</td>\n",
       "      <td>533</td>\n",
       "      <td>House</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        spacy_noss  Frequency1         nltk_ss  Frequency2       nltk_noss  \\\n",
       "0       MailOnline        8849             NYC        2090             NYC   \n",
       "1         COVID-19        6374      MailOnline        1886      MailOnline   \n",
       "2             Ford        5624            VERY        1576            VERY   \n",
       "3           Toyota        5475            Duke        1227            Duke   \n",
       "4          Hyundai        4333              LA        1177              LA   \n",
       "5        Instagram        4023           Queen        1106           Queen   \n",
       "6            Trump        3877              UK         982              UK   \n",
       "7            Honda        3786           COVID         945           COVID   \n",
       "8              BMW        3783      Conditions         881      Conditions   \n",
       "9           Amazon        3595             THE         738             THE   \n",
       "10         Netflix        3406  Princess Diana         729  Princess Diana   \n",
       "11          Palace        3214              US         716         Duchess   \n",
       "12           COVID        3140         Duchess         708              US   \n",
       "13              EV        2970   Prince Philip         702   Prince Philip   \n",
       "14  Britney Spears        2734           LACMA         618           LACMA   \n",
       "15           House        2437        Princess         580         Contact   \n",
       "16             BBC        2427         Contact         551             SUV   \n",
       "17          Nissan        2262             SUV         550      Astroworld   \n",
       "18            Duke        2226           House         542             BBC   \n",
       "19       Chevrolet        2096      Astroworld         533           House   \n",
       "\n",
       "    Frequency3  \n",
       "0         2090  \n",
       "1         1886  \n",
       "2         1578  \n",
       "3         1222  \n",
       "4         1178  \n",
       "5         1106  \n",
       "6          981  \n",
       "7          945  \n",
       "8          881  \n",
       "9          779  \n",
       "10         745  \n",
       "11         726  \n",
       "12         706  \n",
       "13         702  \n",
       "14         618  \n",
       "15         551  \n",
       "16         550  \n",
       "17         533  \n",
       "18         526  \n",
       "19         521  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename the column 1 as 'spacy_noss' second column as 'Frequency' in the dataframe news_text_spacy_noss\n",
    "news_text_spacy_noss = news_text_spacy_noss.rename(columns={news_text_spacy_noss.columns[0]: 'spacy_noss', news_text_spacy_noss.columns[1]: 'Frequency1'})\n",
    "# rename the column 1 as 'nltk_ss' second column as 'Frequency' in the dataframe  news_text_nltk_ss\n",
    "news_text_nltk_ss = news_text_nltk_ss.rename(columns={news_text_nltk_ss.columns[0]: 'nltk_ss', news_text_nltk_ss.columns[1]: 'Frequency2'})\n",
    "# rename the column 1 as 'nltk_noss' second column as 'Frequency' in the dataframe news_text_nltk_noss\n",
    "news_text_nltk_noss = news_text_nltk_noss.rename(columns={news_text_nltk_noss.columns[0]: 'nltk_noss', news_text_nltk_noss.columns[1]: 'Frequency3'})\n",
    "\n",
    "# concat the three dataframes together\n",
    "news_text_combine = pd.concat([news_text_spacy_noss, news_text_nltk_ss, news_text_nltk_noss], axis=1)\n",
    "news_text_combine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Spacy with no sentence segmentation is the best model for news text. The Company name that mentioned the most is \"Ford\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacy_noss_location</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>nltk_ss_location</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>nltk_noss_location</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LA</td>\n",
       "      <td>18382</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2194</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYC</td>\n",
       "      <td>11070</td>\n",
       "      <td>London</td>\n",
       "      <td>2071</td>\n",
       "      <td>London</td>\n",
       "      <td>2070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UK</td>\n",
       "      <td>10682</td>\n",
       "      <td>New York City</td>\n",
       "      <td>1615</td>\n",
       "      <td>New York City</td>\n",
       "      <td>1615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>London</td>\n",
       "      <td>10504</td>\n",
       "      <td>British</td>\n",
       "      <td>1322</td>\n",
       "      <td>British</td>\n",
       "      <td>1322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>9930</td>\n",
       "      <td>New York</td>\n",
       "      <td>1275</td>\n",
       "      <td>New York</td>\n",
       "      <td>1275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US</td>\n",
       "      <td>9855</td>\n",
       "      <td>West</td>\n",
       "      <td>1089</td>\n",
       "      <td>West</td>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New York City</td>\n",
       "      <td>7231</td>\n",
       "      <td>California</td>\n",
       "      <td>970</td>\n",
       "      <td>California</td>\n",
       "      <td>968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>5907</td>\n",
       "      <td>India</td>\n",
       "      <td>955</td>\n",
       "      <td>India</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Australia</td>\n",
       "      <td>5104</td>\n",
       "      <td>Miami</td>\n",
       "      <td>921</td>\n",
       "      <td>Miami</td>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>India</td>\n",
       "      <td>5015</td>\n",
       "      <td>Australia</td>\n",
       "      <td>902</td>\n",
       "      <td>Australia</td>\n",
       "      <td>895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Miami</td>\n",
       "      <td>4842</td>\n",
       "      <td>Malibu</td>\n",
       "      <td>836</td>\n",
       "      <td>Malibu</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>West Hollywood</td>\n",
       "      <td>4833</td>\n",
       "      <td>Britain</td>\n",
       "      <td>779</td>\n",
       "      <td>Britain</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Meghan</td>\n",
       "      <td>4762</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>770</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>New York</td>\n",
       "      <td>4674</td>\n",
       "      <td>American</td>\n",
       "      <td>759</td>\n",
       "      <td>American</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>California</td>\n",
       "      <td>4403</td>\n",
       "      <td>Paris</td>\n",
       "      <td>750</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Beverly Hills</td>\n",
       "      <td>4387</td>\n",
       "      <td>Prince</td>\n",
       "      <td>750</td>\n",
       "      <td>America</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>4226</td>\n",
       "      <td>New</td>\n",
       "      <td>743</td>\n",
       "      <td>Paris</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Britain</td>\n",
       "      <td>3910</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>712</td>\n",
       "      <td>China</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>China</td>\n",
       "      <td>3771</td>\n",
       "      <td>America</td>\n",
       "      <td>705</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Malibu</td>\n",
       "      <td>3648</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>693</td>\n",
       "      <td>Prince Philip</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spacy_noss_location  Frequency nltk_ss_location  Frequency  \\\n",
       "0                   LA      18382      Los Angeles       2194   \n",
       "1                  NYC      11070           London       2071   \n",
       "2                   UK      10682    New York City       1615   \n",
       "3               London      10504          British       1322   \n",
       "4          Los Angeles       9930         New York       1275   \n",
       "5                   US       9855             West       1089   \n",
       "6        New York City       7231       California        970   \n",
       "7            Hollywood       5907            India        955   \n",
       "8            Australia       5104            Miami        921   \n",
       "9                India       5015        Australia        902   \n",
       "10               Miami       4842           Malibu        836   \n",
       "11      West Hollywood       4833          Britain        779   \n",
       "12              Meghan       4762             U.S.        770   \n",
       "13            New York       4674         American        759   \n",
       "14          California       4403            Paris        750   \n",
       "15       Beverly Hills       4387           Prince        750   \n",
       "16              Sydney       4226              New        743   \n",
       "17             Britain       3910           Sydney        712   \n",
       "18               China       3771          America        705   \n",
       "19              Malibu       3648         Facebook        693   \n",
       "\n",
       "   nltk_noss_location  Frequency  \n",
       "0         Los Angeles       2194  \n",
       "1              London       2070  \n",
       "2       New York City       1615  \n",
       "3             British       1322  \n",
       "4            New York       1275  \n",
       "5                West       1083  \n",
       "6          California        968  \n",
       "7               India        951  \n",
       "8               Miami        921  \n",
       "9           Australia        895  \n",
       "10             Malibu        836  \n",
       "11            Britain        779  \n",
       "12               U.S.        767  \n",
       "13           American        757  \n",
       "14             Sydney        707  \n",
       "15            America        700  \n",
       "16              Paris        689  \n",
       "17              China        689  \n",
       "18           Facebook        688  \n",
       "19      Prince Philip        684  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text_spacy_noss_loc_show = news_text_spacy_noss_loc.reset_index()\n",
    "news_text_spacy_noss_loc_show.columns = ['spacy_noss_location', 'Frequency']\n",
    "\n",
    "news_text_nltk_ss_loc_show = news_text_nltk_ss_loc.reset_index()\n",
    "news_text_nltk_ss_loc_show.columns = ['nltk_ss_location', 'Frequency']\n",
    "\n",
    "news_text_nltk_noss_loc_show = news_text_nltk_noss_loc.reset_index()\n",
    "news_text_nltk_noss_loc_show.columns = ['nltk_noss_location', 'Frequency']\n",
    "\n",
    "news_text_combine_loc_show = pd.concat([news_text_spacy_noss_loc_show, news_text_nltk_ss_loc_show, news_text_nltk_noss_loc_show], axis=1)\n",
    "news_text_combine_loc_show.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of NER:\n",
    "\n",
    "1. In this Project, Spacy with no sentence segmentation is the best approch for Name and Entity Recognition\n",
    "2. For tweets, the most frequently mentioned company under the best model is \"Land Rover\"\n",
    "3. For news title, the most frequently mentioned company under the best model is \"Ford\"\n",
    "4. For news text, the most frequently mentioned company under the best model is \"Ford\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Identify what other companies are most frequently mentioned along with your primary company\n",
    "- Analyze what companies are most frequently mentioned within the same document (tweet and news article)\n",
    "- While analyzing news articles, extract separate entities from titles and texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for NER, the best model is Spacy without sentence segmentation, I will use Spacy to do the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "indexlist = []\n",
    "entities = []\n",
    "labels = []\n",
    "\n",
    "docs = nlp.pipe(\n",
    "    news_df['text_new'].tolist(),\n",
    "    disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"],\n",
    "    batch_size=100,\n",
    "    n_process=2\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    index = news_df.index[i]\n",
    "    for ent in doc.ents:\n",
    "        indexlist.append(index)\n",
    "        entities.append(ent.text)\n",
    "        labels.append(ent.label_)\n",
    "\n",
    "# create a dataframe to store the entities and labels and index\n",
    "entities_labels_index = pd.DataFrame({'Index': indexlist, 'Entities': entities, 'Labels': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2498249</th>\n",
       "      <td>10011</td>\n",
       "      <td>the YearStreet Machine</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498248</th>\n",
       "      <td>10011</td>\n",
       "      <td>the Product Safety Australia</td>\n",
       "      <td>ORG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498246</th>\n",
       "      <td>10011</td>\n",
       "      <td>the Nissan Customer Service Centre</td>\n",
       "      <td>ORG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Index                            Entities Labels  Occurences\n",
       "2498249  10011              the YearStreet Machine    ORG           2\n",
       "2498248  10011        the Product Safety Australia    ORG           1\n",
       "2498246  10011  the Nissan Customer Service Centre    ORG           1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER_byindex = entities_labels_index.groupby(['Index', 'Entities', 'Labels']).size().reset_index(name='Occurences')\n",
    "NER_byindex = NER_byindex[NER_byindex['Labels'] == 'ORG']\n",
    "NER_byindex.sort_values(by=['Index', 'Entities', 'Labels'], ascending=False, inplace=True)\n",
    "NER_byindex.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Tweets, we have \"Land Rover\" as the most frequently mentioned company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Land Rover\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occurences</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entities</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Land Rover</th>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PrintsOur</th>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TeamAdvertise</th>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the Daily Mail</th>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ArchiveTopics</th>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Associated Newspapers LtdPart</th>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LocationPublished</th>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Mail</th>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COVID-19</th>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instagram</th>\n",
       "      <td>519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilson</th>\n",
       "      <td>428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBC</th>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Duke</th>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Privacy Policy</th>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook</th>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MailOnline</th>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netflix</th>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trump</th>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon</th>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HBO</th>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Occurences\n",
       "Entities                                 \n",
       "Land Rover                           1290\n",
       "PrintsOur                             564\n",
       "TeamAdvertise                         554\n",
       "the Daily Mail                        553\n",
       "ArchiveTopics                         551\n",
       "Associated Newspapers LtdPart         551\n",
       "LocationPublished                     551\n",
       "The Mail                              551\n",
       "COVID-19                              545\n",
       "Instagram                             519\n",
       "Wilson                                428\n",
       "BBC                                   407\n",
       "Duke                                  406\n",
       "Privacy Policy                        402\n",
       "Facebook                              401\n",
       "MailOnline                            398\n",
       "Netflix                               396\n",
       "Trump                                 392\n",
       "Amazon                                391\n",
       "HBO                                   382"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the most frequentt mentioned Company name is Land Rover, then get the top 20 entities \n",
    "most_freq_company = 'Land Rover' \n",
    "print(most_freq_company)\n",
    "\n",
    "news_id_toyota = NER_byindex[NER_byindex.Entities == most_freq_company]['Index'].unique().tolist()\n",
    "NER_byindex_toyota = NER_byindex[NER_byindex['Index'].isin(news_id_toyota)]\n",
    "\n",
    "# drop label\n",
    "NER_byindex_toyota.drop('Labels', axis=1, inplace=True)\n",
    "# group by Index and entities to get the count of entities\n",
    "NER_byindex_toyota_count_news = NER_byindex_toyota.groupby(['Index', 'Entities']).count()\n",
    "# groupby entities to get average count of occurance for each entity\n",
    "NER_byindex_toyota_gb = NER_byindex_toyota_count_news.groupby('Entities').count().sort_values(by='Occurences', ascending=False)\n",
    "NER_byindex_toyota_gb.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both News Text and News Title, we have \"Ford\" as the most frequently mentioned company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ford\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occurences</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entities</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ford</th>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toyota</th>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Honda</th>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hyundai</th>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autopath Technologies Inc.</th>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COVID-19</th>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMW</th>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the Carpages.ca Terms &amp; Conditions</th>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chevrolet</th>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nissan</th>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EV</th>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YoursBuy From Home Available!Tap</th>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexus</th>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RomeoAston</th>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jeep</th>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cadillac</th>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tesla</th>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Audi</th>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax &amp; licensingMake</th>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kia</th>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Occurences\n",
       "Entities                                      \n",
       "Ford                                      1881\n",
       "Toyota                                     838\n",
       "Honda                                      680\n",
       "Hyundai                                    599\n",
       "Autopath Technologies Inc.                 576\n",
       "COVID-19                                   569\n",
       "BMW                                        556\n",
       "the Carpages.ca Terms & Conditions         435\n",
       "Chevrolet                                  421\n",
       "Nissan                                     406\n",
       "EV                                         379\n",
       "YoursBuy From Home Available!Tap           348\n",
       "Lexus                                      327\n",
       "RomeoAston                                 323\n",
       "Jeep                                       318\n",
       "Cadillac                                   310\n",
       "Tesla                                      300\n",
       "Audi                                       292\n",
       "tax & licensingMake                        273\n",
       "Kia                                        266"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the most frequentt mentioned Company name is Ford, then get the top 20 entities \n",
    "most_freq_company = 'Ford' \n",
    "print(most_freq_company)\n",
    "\n",
    "news_id_toyota = NER_byindex[NER_byindex.Entities == most_freq_company]['Index'].unique().tolist()\n",
    "NER_byindex_toyota = NER_byindex[NER_byindex['Index'].isin(news_id_toyota)]\n",
    "\n",
    "# drop label\n",
    "NER_byindex_toyota.drop('Labels', axis=1, inplace=True)\n",
    "# group by Index and entities to get the count of entities\n",
    "NER_byindex_toyota_count_news = NER_byindex_toyota.groupby(['Index', 'Entities']).count()\n",
    "# groupby entities to get average count of occurance for each entity\n",
    "NER_byindex_toyota_gb = NER_byindex_toyota_count_news.groupby('Entities').count().sort_values(by='Occurences', ascending=False)\n",
    "NER_byindex_toyota_gb.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Other Associated Companies Analysis\n",
    "\n",
    "'Land Rover' and 'Ford' are the most frequently mentioned companies in the news and tweets.\n",
    "\n",
    "Some other frequently mentioned companies are car brands such as Toyota and Honda. Also, there are some media company such as the daily mail and BBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identify most frequent locations of events, by extracting appropriate named entities\n",
    "- Locations may include countries, states, cities, regions, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the best model is Spacy without sentence segmentation, I will use Spacy to do the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets_spacy_noss</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>News_title_spacy_noss</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>News_text_spacy_noss</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russia</td>\n",
       "      <td>471</td>\n",
       "      <td>Carpages.ca</td>\n",
       "      <td>1962</td>\n",
       "      <td>LA</td>\n",
       "      <td>18382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK</td>\n",
       "      <td>365</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>1316</td>\n",
       "      <td>NYC</td>\n",
       "      <td>11070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n‚Äôt</td>\n",
       "      <td>208</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>198</td>\n",
       "      <td>UK</td>\n",
       "      <td>10682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>88</td>\n",
       "      <td>UK</td>\n",
       "      <td>194</td>\n",
       "      <td>London</td>\n",
       "      <td>10504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kibaki</td>\n",
       "      <td>76</td>\n",
       "      <td>Manitoba</td>\n",
       "      <td>181</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>9930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Meghan</td>\n",
       "      <td>72</td>\n",
       "      <td>Winnipeg</td>\n",
       "      <td>137</td>\n",
       "      <td>US</td>\n",
       "      <td>9855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jamaica</td>\n",
       "      <td>66</td>\n",
       "      <td>India</td>\n",
       "      <td>121</td>\n",
       "      <td>New York City</td>\n",
       "      <td>7231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Britain</td>\n",
       "      <td>60</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>118</td>\n",
       "      <td>Hollywood</td>\n",
       "      <td>5907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>41</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>116</td>\n",
       "      <td>Australia</td>\n",
       "      <td>5104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>London</td>\n",
       "      <td>39</td>\n",
       "      <td>London</td>\n",
       "      <td>112</td>\n",
       "      <td>India</td>\n",
       "      <td>5015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>35</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>90</td>\n",
       "      <td>Miami</td>\n",
       "      <td>4842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>China</td>\n",
       "      <td>33</td>\n",
       "      <td>US</td>\n",
       "      <td>84</td>\n",
       "      <td>West Hollywood</td>\n",
       "      <td>4833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Snatch Land Rover</td>\n",
       "      <td>29</td>\n",
       "      <td>Saskatchewan</td>\n",
       "      <td>78</td>\n",
       "      <td>Meghan</td>\n",
       "      <td>4762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Hague</td>\n",
       "      <td>29</td>\n",
       "      <td>North York</td>\n",
       "      <td>72</td>\n",
       "      <td>New York</td>\n",
       "      <td>4674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MeghanMarkle</td>\n",
       "      <td>28</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>64</td>\n",
       "      <td>California</td>\n",
       "      <td>4403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>25</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>64</td>\n",
       "      <td>Beverly Hills</td>\n",
       "      <td>4387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>France</td>\n",
       "      <td>25</td>\n",
       "      <td>China</td>\n",
       "      <td>59</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>4226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>24</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>54</td>\n",
       "      <td>Britain</td>\n",
       "      <td>3910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Nyeri</td>\n",
       "      <td>23</td>\n",
       "      <td>Kitchener</td>\n",
       "      <td>45</td>\n",
       "      <td>China</td>\n",
       "      <td>3771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ukraine</td>\n",
       "      <td>22</td>\n",
       "      <td>Australia</td>\n",
       "      <td>44</td>\n",
       "      <td>Malibu</td>\n",
       "      <td>3648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tweets_spacy_noss  Frequency News_title_spacy_noss  Frequency  \\\n",
       "0              Russia        471   Carpages.ca               1962   \n",
       "1                  UK        365               Ontario       1316   \n",
       "2                 n‚Äôt        208      British Columbia        198   \n",
       "3               India         88                    UK        194   \n",
       "4              Kibaki         76              Manitoba        181   \n",
       "5              Meghan         72              Winnipeg        137   \n",
       "6             Jamaica         66                 India        121   \n",
       "7             Britain         60               Toronto        118   \n",
       "8            Zimbabwe         41               Alberta        116   \n",
       "9              London         39                London        112   \n",
       "10       South Africa         35             Cambridge         90   \n",
       "11              China         33                    US         84   \n",
       "12  Snatch Land Rover         29          Saskatchewan         78   \n",
       "13          The Hague         29            North York         72   \n",
       "14       MeghanMarkle         28               Calgary         64   \n",
       "15        New Zealand         25                  U.S.         64   \n",
       "16             France         25                 China         59   \n",
       "17          Hollywood         24                Taiwan         54   \n",
       "18              Nyeri         23             Kitchener         45   \n",
       "19            Ukraine         22             Australia         44   \n",
       "\n",
       "   News_text_spacy_noss  Frequency  \n",
       "0                    LA      18382  \n",
       "1                   NYC      11070  \n",
       "2                    UK      10682  \n",
       "3                London      10504  \n",
       "4           Los Angeles       9930  \n",
       "5                    US       9855  \n",
       "6         New York City       7231  \n",
       "7             Hollywood       5907  \n",
       "8             Australia       5104  \n",
       "9                 India       5015  \n",
       "10                Miami       4842  \n",
       "11       West Hollywood       4833  \n",
       "12               Meghan       4762  \n",
       "13             New York       4674  \n",
       "14           California       4403  \n",
       "15        Beverly Hills       4387  \n",
       "16               Sydney       4226  \n",
       "17              Britain       3910  \n",
       "18                China       3771  \n",
       "19               Malibu       3648  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename the column 1 as 'Tweets_spacy_noss' for tweets_spacy_noss_loc_show\n",
    "tweets_spacy_noss_loc_show = tweets_spacy_noss_loc.reset_index()\n",
    "tweets_spacy_noss_loc_show.columns = ['Tweets_spacy_noss', 'Frequency']\n",
    "\n",
    "# rename the column 1 as 'news_title_spacy_noss' for news_title_spacy_noss_loc_show\n",
    "news_title_spacy_noss_loc_show = news_title_spacy_noss_loc.reset_index()\n",
    "news_title_spacy_noss_loc_show.columns = ['News_title_spacy_noss', 'Frequency']\n",
    "\n",
    "# rename the column 1 as 'news_text_spacy_noss' for news_text_spacy_noss_loc_show\n",
    "news_text_spacy_noss_loc_show = news_text_spacy_noss_loc.reset_index()\n",
    "news_text_spacy_noss_loc_show.columns = ['News_text_spacy_noss', 'Frequency']\n",
    "# create a dataframe concact the tweets_nltk_noss_loc_show, news_title_spacy_noss_loc_show, news_text_spacy_noss_loc_show\n",
    "df_loc = pd.concat([tweets_spacy_noss_loc_show, news_title_spacy_noss_loc_show, news_text_spacy_noss_loc_show], axis=1)\n",
    "df_loc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Location Analysis\n",
    "\n",
    "For Tweets, we have **\"Russia\"** as the most frequently mentioned location\n",
    "\n",
    "For News Title, we have **\"Ontario\"** as the most frequently mentioned location\n",
    "\n",
    "For News Text, we have **\"LA\"** as the most frequently mentioned location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created at: Tue, 25 April 2023 21:17:04 by Richard Yang\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "time = datetime.datetime.now(pytz.timezone('US/Central')).strftime(\"%a, %d %B %Y %H:%M:%S\")\n",
    "sign = 'Richard Yang'\n",
    "\n",
    "print(f'Created at: {time} by {sign}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
